# v1 = rbind(mean(log_test[,1]), sd(log_test[,1]))
#
# # Find the means and SDs
# v2 = rbind(mean(log_test[,2]), sd(log_test[,2]))
#
# # Store as a data frame
# sum_df = as.data.frame(cbind(v1,v2))
# rownames(sum_df) <- c("Mean", "SD")
# colnames(sum_df) <- c("No AR", "SAV")
#
# # Format nicely
# sum_df %>% kable(caption = "Summary Statistics for the Log of Losses from Each Model", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling()
# log_test
# this is the function Pr(X_t | Z_t=k) for our example
# This is the emission probability in the "rational world", modified for whether we miss positive or negative
emit_norm = function(obs,mean, sd){
dnorm(obs, mean = mean, sd = sd)
# dnorm(x,mean=k,sd=sd)
}
# Here is where we will input our emission probabilities for a GEV
emit_gev = function(obs,loc, scale, shape){
dgev(obs, loc = loc, scale = scale, shape = shape)
}
# The GEV doesn't have support over all of the real line, which is a problem
emit_sn = function(obs,loc, scale, shape){
dsnorm(obs, mean = loc, sd = scale, xi = shape)
}
# Let's try the Gumbel distribution
emit_gumb = function(obs,loc, scale){
dgumbel(obs, loc = loc, scale = scale)
}
# sn_param_df
# Our prior is that both states are equally likely.
prior = c(0.5,0.5) #Assumed prior distribution on Z_1
# The matrix where we store the forwards probabilities
alpha = matrix(nrow = T,ncol=K)
# for(k in 1:K){
#   alpha[1,k] = prior[k] * emit(k,X[1])
# }
# head(small_df)
# norm_param_df$`Standard Deviation`
# gev_param_df$Location
# sn_param_df$Location
gum_param_df = ev_param_df[2,1:2]
# gum_param_df$Location
# Initialize alpha[1,]
for(k in 1:K){
if (k == 1){
# This is the emission probability assuming normality
alpha[1,k] = prior[k] * emit_norm(obs = small_df$SPY[1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
} else if (k == 2){
# This is the emission probability assuming GEV
# alpha[1,k] = prior[k] * emit_gev(obs = small_df$SPY[1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
# This is the emission probability assuming SN
# alpha[1,k] = prior[k] * emit_sn(obs = small_df$SPY[1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
# Third time is the charm...assume a Gumbel
alpha[1,k] = prior[k] * emit_gumb(obs = small_df$SPY[1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
} else {
print("k should not be greater than 2")
}
}
# head(alpha)
# small_df$SPY[1]
# alpha[1,] %*% P
# Forward algorithm
for(t in 1:(T-1)){
# Find the value of m at each step
m = alpha[t,] %*% P
# Loop through to update levels of alpha
for(k in 1:K){
# alpha[t+1,k] = m[k]*emit(k,X[t]) what I had
# alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
if (k == 1){
# alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
alpha[t+1,k] = m[k]*emit_norm(obs = small_df$SPY[t+1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
} else if (k == 2){
# Assuming a GEV
# alpha[t+1,k] = m[k]*emit_gev(obs = small_df$SPY[t+1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
# Assuming a skew-normal
# alpha[t+1,k] = m[k]*emit_sn(obs = small_df$SPY[t+1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
# Assuming a Gumbel
alpha[t+1,k] = m[k]*emit_gumb(obs = small_df$SPY[t+1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
} else {
print("k should not be greater than 2")
}
# alpha[t+1,k] = m[k]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k])
}
}
# emit_gev(obs = small_df$SPY[2], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
# emit_sn(obs = small_df$SPY[2], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
# sn_param_df$Location
# sn_param_df$Scale
# sn_param_df$Shape
# dsnorm(small_df$SPY[2], mean = sn_param_df$Location, sd = sn_param_df$Scale, xi = sn_param_df$Shape)
# hist(rsnorm(10000, mean = sn_param_df$Location, sd = sn_param_df$Scale, xi = sn_param_df$Shape), xlim = c(-2,0))
# Try it with a gumbel
# dgumbel(small_df$SPY[3], loc = gum_param_df$Location, scale = gum_param_df$Scale)
# small_df$SPY[2]
# alpha
# Initalize a beta matrix
beta = matrix(nrow = T,ncol=K)
# Initialize beta
for(k in 1:K){
beta[T,k] = 1
}
# Backwards algorithm
for(t in (T-1):1){
for(k in 1:K){
# Modify the value of beta
# beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
# beta[t,k] = sum(beta[t+1,]*P[k,]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k]))
# beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
if (k == 1){
# Here's what was online
# beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
# Find the values separately
v1 = emit_norm(obs = small_df$SPY[t+1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
v2 = emit_gumb(obs = small_df$SPY[t+1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
beta[t,k] = sum(beta[t+1,]*P[k,]*c(v1, v2))
} else if (k == 2){
# Here's what was online
# Find the values separately
v1 = emit_norm(obs = small_df$SPY[t+1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
v2 = emit_gumb(obs = small_df$SPY[t+1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
beta[t,k] = sum(beta[t+1,]*P[k,]*c(v1, v2))
} else {
print("k should not be greater than 2")
}
}
}
# Lastly, we need to solve for the posterior
ab = alpha*beta
prob = ab/rowSums(ab)
# rowSums(prob)
# prob
plot(prob[,2],type="l",ylim=c(0,1), main="Chance the World is in an Irrational State",lwd=2,ylab="Posterior Probability")
# lines(Z==2,col=2,lwd=2)
# Bolt on an NA
norm_param_df2 = cbind(norm_param_df, NA)
# Rename rows and columns
rownames(norm_param_df2) <- c("Normal")
colnames(norm_param_df2) <- c("Location", "Scale", "Shape")
# Combine into an optimal parameter df
opt_param_df = rbind(norm_param_df2, ev_param_df)
opt_param_df$Location
opt_param_df
abc = opt_param_df[c("Normal"),]
abc$Location
#' A function to fit multiple HMMs given an input dataset
#'
#' @param input_df - input dataframe
#' @param t_prob - transition probability
#' @param prior_prob - prior probability
#' @param irr_dist - the irrational distribution. Right now, this code is only set up to work with Gumbel
#' @param rat_dist - rational distribution. Right now, only designed for normal.
#' @param rat_opt_param - optimal paramaters for the rational distribution. For 2008, it was a normal.
#' @param irr_opt_param - optimal parameters for the irrational distribution. For 2008, it was a Gumbel
#' @param var_level - the VaR level used in the plot
#'
#' @return - the probabilities in a nicely formatted table
#' @export
#'
#' @examples - hmm_fit(input_df = small_df, t_prob = 0.1, prior_prob = 0.5, irr_dist = "Gumbel")
hmm_fit = function(input_df, t_prob, prior_prob, rat_opt_param, irr_opt_param, irr_dist = "Gumbel", rat_dist = "Normal", var_level){
# Set a value of T and K
T = nrow(input_df)
K = ncol(input_df) - 1
# Initialize optimal parameters for rational and irrational datasets
rat_opt_param = rat_opt_param[c(rat_dist),]
irr_opt_param = irr_opt_param[c(irr_dist),]
# print(rat_opt_param)
# Encode the prior
prior = c(prior_prob, 1- prior_prob) #Assumed prior distribution on Z_1
# Assign a value of P - the transition matrix
P = cbind(c(1-t_prob,t_prob),c(t_prob,1-t_prob))
# print("Stop 1")
# Initialize alpha[1,]
for(k in 1:K){
if (k == 1){
# This is the emission probability in the "rational" world
alpha[1,k] = prior[k] * emit_norm(obs = input_df$SPY[1], mean = rat_opt_param$Location, sd = rat_opt_param$Scale)
} else if (k == 2){
# This is the emission probability assuming GEV
# alpha[1,k] = prior[k] * emit_gev(obs = small_df$SPY[1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
# This is the emission probability assuming SN
# alpha[1,k] = prior[k] * emit_sn(obs = small_df$SPY[1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
# Third time is the charm...assume a Gumbel
alpha[1,k] = prior[k] * emit_gumb(obs = input_df$SPY[1], loc = irr_opt_param$Location, scale = irr_opt_param$Scale)
} else {
print("k should not be greater than 2")
}
}
# Forward algorithm
for(t in 1:(T-1)){
# Find the value of m at each step
m = alpha[t,] %*% P
# Loop through to update levels of alpha
for(k in 1:K){
# alpha[t+1,k] = m[k]*emit(k,X[t]) what I had
# alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
if (k == 1){
# alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
alpha[t+1,k] = m[k]*emit_norm(obs = input_df$SPY[t+1], mean = rat_opt_param$Location, sd = rat_opt_param$Scale)
} else if (k == 2){
# Assuming a GEV
# alpha[t+1,k] = m[k]*emit_gev(obs = small_df$SPY[t+1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
# Assuming a skew-normal
# alpha[t+1,k] = m[k]*emit_sn(obs = small_df$SPY[t+1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
# Assuming a Gumbel
alpha[t+1,k] = m[k]*emit_gumb(obs = input_df$SPY[t+1], loc = irr_opt_param$Location, scale = irr_opt_param$Scale)
} else {
print("k should not be greater than 2")
}
# alpha[t+1,k] = m[k]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k])
}
}
# Initalize a beta matrix
beta = matrix(nrow = T,ncol=K)
# Initialize beta
for(k in 1:K){
beta[T,k] = 1
}
# print("Stop 2")
# Backwards algorithm
for(t in (T-1):1){
for(k in 1:K){
# Modify the value of beta
# beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
# beta[t,k] = sum(beta[t+1,]*P[k,]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k]))
# beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
if (k == 1){
# Here's what was online
# beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
# Find the values separately
v1 = emit_norm(obs = input_df$SPY[t+1], mean = rat_opt_param$Location, sd = rat_opt_param$Scale)
v2 = emit_gumb(obs = input_df$SPY[t+1], loc = irr_opt_param$Location, scale = irr_opt_param$Scale)
beta[t,k] = sum(beta[t+1,]*P[k,]*c(v1, v2))
} else if (k == 2){
# Here's what was online
# Find the values separately
v1 = emit_norm(obs = input_df$SPY[t+1],  mean = rat_opt_param$Location, sd = rat_opt_param$Scale)
v2 = emit_gumb(obs = input_df$SPY[t+1],  loc = irr_opt_param$Location, scale = irr_opt_param$Scale)
beta[t,k] = sum(beta[t+1,]*P[k,]*c(v1, v2))
} else {
print("k should not be greater than 2")
}
}
}
# Lastly, we need to solve for the posterior
ab = alpha*beta
# print(prob)
prob = ab/rowSums(ab)
# Plot the Data
plot(prob[,2],type="l",ylim=c(0,1), main="Chance the World is in an Irrational State",lwd=2,ylab="Posterior Probability",
xlab = paste0("The 250 Trading Days from ", min(index(input_df)), " to ", max(index(input_df))), sub = paste0("The Probability of Transition is ", t_prob*100, "%; VaR Level is ", var_level, "%"))
# Beautify the probabilities
prob_df = as.data.frame(prob)
# Bolt on dates
# prob_df = cbind(index(input_df), prob_df)
# Add column names
colnames(prob_df) <- c("Rational", "Irrational")
# print(dim(input_df))
# print(length(index(input_df)))
# print(prob_df)
# Add rownames
rownames(prob_df) <- index(input_df)
# colnames(prob_df) <- c("Dates","Rational", "Irrational")
# print(cbind(input_df, prob_df))
return(list(probs = prob_df, alpha = alpha, beta = beta))
}
out = hmm_fit(input_df = small_df, t_prob = 0.1, prior_prob = 0.5, irr_dist = "Gumbel", rat_opt_param = norm_param_df2, irr_opt_param = ev_param_df, var_level = 1)
# head(out)
# rowSums(out)
# index(small_df)
# cbind(index(small_df), out)
# Call the above function to import data from my thesis
v1_2008_alletf1 = var_input_disp("/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_all_ETF_runs/","var_1pc_2008_all_etf.csv", 0.01, print_graph = 0, print_mv_table = 0, print_uv_table = 0, print_opt_param =0)
v1_2008_alletf2 = var_input_disp("/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_all_ETF_runs/","var_1pc_2008_all_etf.csv", 0.05, print_graph = 0, print_mv_table = 0, print_uv_table = 0, print_opt_param =0)
#' This is a function to read in the losses from previous years
#'
#' @param var_level - the VaR level as an integer
#' @param year - the year to read in. 2008, 2010, 2014, and 2016.
#' @param path - the path to the files. You probably shouldn't change this.
#'
#' @return - the losses by year
#' @export
#'
#' @examples - import_dat(1, 2008)
import_dat = function(var_level, year, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_all_ETF_runs/"){
# Import the data frame
int_df = var_input_disp(path, paste0("var_", var_level, "pc_", year, "_all_etf.csv"), 0.01, print_graph = 0, print_mv_table = 0, print_uv_table = 0, print_opt_param =0)
# Extract the losses
losses = gen_loss_test(data_mat = int_df[[1]], tau = var_level/100)[[3]]
# Assign the predictions
preds = int_df[[1]]
# Return the losses
return(list(preds = preds, losses = losses))
}
# v1_2008_alletf
dat1 = import_dat(1, 2008)
# abc
# abcde =colSums(abc)
# abcde
#
# which.min(abcde[1:8])
#' A function that chooses the optimal models
#'
#' @param loss_mat - loss matrix from previous code
#'
#' @return - the optimal models for the HMM
#' @export
#'
#' @examples - pick_best(dat1)
pick_best = function(loss_mat){
# Find the column sums
cs = colSums(loss_mat)
# Subset into two - rational and irrational parts (split in two)
rat_sum = cs[1:4]
irr_sum = cs[5:8]
# Find the minimizer of each set
rat_min = which.min(rat_sum)
# Add 4 for the offset
irr_min = which.min(irr_sum) + 4
# print(c(rat_min, irr_min))
# Return the optimal pairs
return(c(rat_min, irr_min))
}
best = pick_best(dat1$losses)
c(1,best+1)
# These are the losses generated from our final run
# test_loss = gen_loss_test(data_mat = v1_2008_alletf[[1]], tau = 0.01)[[3]]
# Subset the data
#' A function to subset the data and print the optimal models
#'
#' @param opt_params - the optimal parameters from the pick_best function
#' @param init_df - the initial data frame to read off of
#' @param print_best_mods - print the best models that will go through the optimization process
#'
#' @return - a subsetted data frame
#' @export
#'
#' @examples - test_df = small_df_fn(best, dat1$preds, print_best_mods = 1)
small_df_fn = function(opt_params, init_df, print_best_mods = 0){
# Select the SP500 and the optimal columsn from earlier
small_df= init_df[,c(1,opt_params+1)]
# Print the optimal parameters
if (print_best_mods != 0){
print(paste0("The best multivariate CAViaR model for the time period from ",
min(index(small_df)), " to ", max(index(small_df))," is ", colnames(small_df)[2],
" and the best univariate CAViaR model for the time period is ", colnames(small_df)[3]))
}
# Return the data frame
return(small_df)
}
small_df1 = small_df_fn(best, dat1$preds, print_best_mods = 1)
# test_df = small_df_fn(best, dat1$preds, print_best_mods = 1)
#' Fit models for the univariate CAViaR models
#'
#' @param input_df - the input data for the univariate CAViaR Model. It should be from the small_df_fn above
#'
#' @return - a 2x4 table with the optimal parameters for skew normal and Gumbel and the K-L divergence for each
#' @export
#'
#' @examples - uv_cav_fit(small_df1)
uv_cav_fit = function(input_df){
# Fit skew normal
sn_param = snormFit(input_df[,3])
# Fit Gumbel
gum_param = gum.fit(input_df[,3], show = FALSE)
# Assign to a data frame
sn_param_df = as.data.frame(t(sn_param$par))
gum_param_df = as.data.frame(t(c(gum_param$mle,NA)))
# Assign column names
colnames(sn_param_df) <- c("Location", "Scale", "Shape")
colnames(gum_param_df) <- c("Location", "Scale", "Shape")
# Plot the data
h = hist(input_df[,3], breaks = 25, xlab = "Modeled Values", main = "Univariate CAViaR")
# Overset a density function - Skew Normal
xfit<-seq(min(input_df[,3]),max(input_df[,3]),length=250)
yfit = dsnorm(xfit, mean = sn_param$par[1], sd = sn_param$par[2], xi = sn_param$par[3])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
lines(xfit, yfit, col="blue", lwd=2)
# Overset a density function - Gumbel
xfit<-seq(min(input_df[,3]),max(input_df[,3]),length=250)
yfit = dgumbel(xfit, loc = gum_param$mle[1], scale = gum_param$mle[2])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
lines(xfit, yfit, col="red", lwd=2)
# Overset a density function - GEV
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="green", lwd=2)
# Add a legend
legend("topleft", legend = c("Skew Normal", "Gumbel"), col = c("blue", "red"), lty = c(1,1), lwd = c(2,2))
# Combine all of the extreme value distributions together
ev_param_df = as.data.frame(rbind(sn_param_df, gum_param_df))
rownames(ev_param_df) <- c("Skew Normal", "Gumbel")
colnames(ev_param_df) <- c("Location", "Scale", "Shape")
# # Display a pretty table
print(ev_param_df %>% kable(caption = "Optimal Parameters for the Candidate Distributions", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood"))
# Compute the K-L divergences
# First, find the density of the data
den_obj = hist(small_df[,3], breaks = 25, plot = FALSE)
# Rewrite m2 as the optimal parameters of skew normal
# Next compute the probability at each point for the fitted models
## Skew-normal
skn_den = dsnorm(den_obj$mids, mean = m2$par[1], sd = m2$par[2], xi = m2$par[3])
## Gumbel
gumb_den = dgumbel(den_obj$mids, loc = gum_param$mle[1], scale = gum_param$mle[2])
## GEV
# gev_den = dgev(den_obj$mids, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
# Calculate the K-L divergences for each
kl_skn = KLD(den_obj$density, skn_den)
kl_gumb = KLD(den_obj$density, gumb_den)
# kl_gev = KLD(m3_den$density, gev_den)
# Combine into a data frame
kl_df = as.data.frame(cbind(kl_skn$mean.sum.KLD, kl_gumb$mean.sum.KLD))
rownames(kl_df) <- c("Mean Sum K-L Divergence")
colnames(kl_df) <- c("Skew-Normal", "Gumbel")
# Display a pretty table
print(kl_df %>% kable(caption = "Comparing K-L Divergences By Model Fits", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling())
# Create a final output dataframe
output_df = as.data.frame(cbind(ev_param_df, t(kl_df)))
# Rename the lastname
colnames(output_df)[4] <- c("K-L Divergence")
# Return the optimal data
return(output_df)
}
abc = uv_cav_fit(small_df1)
# small_df1
# abc
#' Fit models for the univariate CAViaR models
#'
#' @param input_df - the input data for the multivariate CAViaR Model. It should be from the small_df_fn above
#'
#' @return - a 2x4 table with the optimal parameters for skew normal and Gumbel and the K-L divergence for each
#' @export
#'
#' @examples - uv_cav_fit(small_df1)
mv_cav_fit = function(input_df){
# Estimate the mle of the SD
mean_mle = mean(input_df[,2])
sd_mle = (sum((input_df[,2] - mean_mle)^2)/length(input_df[,2]))^(1/2)
# Fit a normal
norm_param = c(mean_mle, sd_mle)
# Fit skew normal
sn_param = snormFit(input_df[,2])
# Assign to a data frame
norm_param_df = as.data.frame(t(c(norm_param,NA)))
sn_param_df = as.data.frame(t(sn_param$par))
# Assign column names
colnames(norm_param_df) <- c("Location", "Scale", "Shape")
colnames(sn_param_df) <- c("Location", "Scale", "Shape")
# Plot the data
h = hist(input_df[,2], breaks = 25, xlab = "Modeled Values", main = "Multivariate CAViaR")
# Overset a density function - Normal
xfit<-seq(min(input_df[,2]),max(input_df[,2]),length=250)
yfit = dnorm(xfit, mean = norm_param[1], sd = norm_param[2])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,2])
lines(xfit, yfit, col="red", lwd=2)
# Overset a density function - Skew Normal
xfit<-seq(min(input_df[,2]),max(input_df[,2]),length=250)
yfit = dsnorm(xfit, mean = sn_param$par[1], sd = sn_param$par[2], xi = sn_param$par[3])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,2])
lines(xfit, yfit, col="blue", lwd=2)
# Overset a density function - GEV
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="green", lwd=2)
# Add a legend
legend("topleft", legend = c("Normal", "Skew Normal"), col = c("blue", "red"), lty = c(1,1), lwd = c(2,2))
# Combine all of the extreme value distributions together
ev_param_df = as.data.frame(rbind(norm_param_df, sn_param_df))
rownames(ev_param_df) <- c("Normal", "Skew Normal")
colnames(ev_param_df) <- c("Location", "Scale", "Shape")
# # Display a pretty table
print(ev_param_df %>% kable(caption = "Optimal Parameters for the Candidate Distributions", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood"))
# Compute the K-L divergences
# First, find the density of the data
den_obj = hist(small_df[,2], breaks = 25, plot = FALSE)
# Rewrite m2 as the optimal parameters of skew normal
# Next compute the probability at each point for the fitted models
## Skew-normal
norm_den = dnorm(den_obj$mids, mean = norm_param[1], sd = norm_param[2])
## Gumbel
# gumb_den = dgumbel(den_obj$mids, loc = gum_param$mle[1], scale = gum_param$mle[2])
skn_den = dsnorm(den_obj$mids, mean = m2$par[1], sd = m2$par[2], xi = m2$par[3])
## GEV
# gev_den = dgev(den_obj$mids, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
# Calculate the K-L divergences for each
kl_norm = KLD(den_obj$density, norm_den)
kl_skn = KLD(den_obj$density, skn_den)
# kl_gev = KLD(m3_den$density, gev_den)
# Combine into a data frame
kl_df = as.data.frame(cbind(kl_norm$mean.sum.KLD, kl_skn$mean.sum.KLD))
rownames(kl_df) <- c("Mean Sum K-L Divergence")
colnames(kl_df) <- c("Normal", "Skew-Normal")
# Display a pretty table
print(kl_df %>% kable(caption = "Comparing K-L Divergences By Model Fits", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling())
# Create a final output dataframe
output_df = as.data.frame(cbind(ev_param_df, t(kl_df)))
# Rename the lastname
colnames(output_df)[4] <- c("K-L Divergence")
# Return the optimal data
return(output_df)
}
def = mv_cav_fit(small_df1)
small_df1[,2]
abc
def
opt_param_df
