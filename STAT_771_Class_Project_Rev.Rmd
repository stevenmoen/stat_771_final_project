---
title: 'STAT 771 - Project Final Draft'
author: "Steven Moen"
date: "Sunday, November 29th, 2020"
output:
  pdf_document: default
  html_notebook: default
bibliography: stat_771_project_bib.bib
link-citations: yes
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.path='Figs/', echo=FALSE, warning=FALSE, message=FALSE, cache = TRUE)
```

# Background and Motivation

My M.S. thesis at the University of Chicago developed a novel method to understand and measure value-at-risk, a commonly accepted method to measure downside risk. The metric is understood as follows: a one-day 1% VaR of -10 million dollars for a portfolio means that the portfolio will lose at least 10 million dollars of its value on the 1% worst trading days. A major advantage of VaR is that it distills a distribution of returns into one number. As such, VaR is often used in stress testing by regulatory agencies [@Holton2014]. 

There have been many popular approaches in the literature to estimate VaR such as modeling the total distribution of returns [@Longerstaey1996] and using a semiparametric or a nonparametric historical simulation [@Richardson2005]. Engle and Manganelli argue in a 2004 paper [@Engle2004] that while modeling the entire distribution is likely too simplistic, nonparametric methods in the other camp are usually chosen for “empirical justifications rather than on sound statistical theory”. To balance these approaches, they propose a framework called CAViaR that directly forecasts the VaR quantile using a conditional autoregressive quantile specification. This approach builds upon the statistical literature that extends linear quantile models to settings amenable to financial modeling, such as with heteroskedastic and nonstationary error distributions [@Portnoy1991].

My thesis extends the model beyond a univariate setting into a multivariate setting using the diffusion index model, originally developed by Stock and Watson for predicting conditional means [@Stock2002; @Stock2002a]. My model uses exchange-traded funds (ETFs) as explanatory variables that are combined into principal component vectors at the forecast origin. Combining these principal component vectors with transformations of lagged autoregressive response variables produces similar predictive accuracy during periods of relatively low volatility (when compared to the CAViaR model) along with more insight into the drivers of the changes in the response variable.

# Intended Work

As encouraging as the results were from my thesis, two important questions remained unanswered. The first is whether some sort of mixture model would be appropriate, that is, aiming to use the basket of ETFs during good times, and use the CAViaR ARMA specification during bad times. The approach of using ETFs allows a prediction based on forward-looking expectations of fundamental factors. Indeed, ETFs are just baskets of individual stocks or bonds, and those securities are (in theory) based on rational expectations about future resources, market conditions, etc - the microfoundations of what drives our economy. The ARMA specification, while practically and statistically sound, is contradicted by economic theory and practice - the weak form of the efficient market hypothesis states that it is impossible to forecast future values of asset prices using past values. But perhaps this view is incomplete. 

To combine these ideas, I will fit a Hidden Markov Model to infer the state of the world - the "rational" one, or the "irrational" one. Given the highly non-normal nature of financial data, I suspect there would be many interesting statistical and computational challenges that would arise with this approach. In addition, it is likely worth exploring alternative ensemble methods to further probe into the seemingly enigmatic nature that pervades financial time series.

When I implement this, the "rational" state of the world will refer to the predictions of one of the multivariate CAViaR models where as the "irrational" state of the world will refer to one of the predictions from the univariate CAViaR models. One way to possibly implement this is to say that if the losses from the multivariate models are lower, then the HMM will lean towards the rational state of the world, otherwise, it will lean towards the other state.


```{r}
# Read in relevant libraries
library(microbenchmark)
library(data.table)
library(quantmod)
library(ggplot2)
library(tseries)
library(zoo)
library(magrittr)
library(dplyr)
library(kableExtra)
library(formattable)
library(quantreg)
library(MTS)
library(plot3D)
library(citr)
library(formattable)
library(fGarch)
library(ismev)
library(evd)
library(LaplacesDemon)


# Set up working directory
source('~/Documents/GitHub/CaviaR/caviar_SM.R')
```





```{r}
#' A function to input the VaR files, plot them and generate tables
#'
#' @param file_path - file path to use
#' @param filename - name of the file
#' @param tau - quantile to use
#' @param resp_var - response variable to use in the plot
#' @param ntest - number of test points
#' @param cn_input - column name inputs
#'
#' @return - a list of the xts file, the plot, the loss list, and tables
#' @export - a plot and tables
#'
#' @examples - test = var_input_disp("/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/","var_1pc_2008_us_etf.csv", 0.01)
var_input_disp = function(file_path, filename, tau, resp_var = "SPY", ntest = 250, cn_input = c("SPY", "No AR", "AR", "SAV AR", "AS AR", "SAV", "Asym. Slope", "Ind. GARCH", "Adaptive"), print_graph = 1, print_mv_table = 1, print_uv_table = 1, print_opt_param =1){
  # Import data
  plot_mat = read.csv(paste0(file_path,filename), sep = ",", header = T, stringsAsFactors = FALSE)
  # Fix date format
  plot_mat$Index = as.Date(plot_mat$Index)
  # Convert to an xts
  plot_mat = xts(plot_mat[,-1], order.by = plot_mat[,1])
  # Fix column names
  colnames(plot_mat) <- cn_input
  # Plot everything
  if (print_graph == 1){
    plt_data(plot_mat, tau = tau, resp_var = resp_var, ntest = ntest)
  }
  # Calculate losses
  l_list = gen_loss_test(plot_mat, tau = tau)
  # Put into tables
  df = as.data.frame(rbind(l_list[[1]], l_list[[2]]))
  # Calculate inital and ending time value
  start = index(plot_mat)[1]
  end = index(plot_mat)[nrow(plot_mat)]
  # Add row/column names
  colnames(df) <- colnames(plot_mat[,-1])
  rownames(df) <- c("Losses", "VaR Break Rate")
  # Edits on 5.12.2020 - divide the table into 2
  mv_df = df[,1:4]
  uv_df = df[,5:8]
  if (print_mv_table == 1){
    print(knitr::kable(mv_df, digits = 3), format = 'pandoc')
    cat("\n")
  }
  if (print_uv_table == 1){
    print(knitr::kable(uv_df, digits = 3), format = 'pandoc')
    cat("\n")
  }
  # Print the optimal parameters
  if (print_opt_param == 1){
    # Import the data frame
    opt_pm = read.csv(paste0(file_path,paste0(substr(filename, 1, nchar(filename)-4),"_pm.csv")), sep = ",", header = T, stringsAsFactors = FALSE)
    opt_pm <- opt_pm[,-1]
    # Assign names
    rownames(opt_pm) <- cn_input[2:5]
    colnames(opt_pm) <- c("Optimal M", "Optimal P")
    # Fix the row and column names
    # Format nicely
    # print(opt_pm %>% kable(caption = "Optimal Number of Diffusion Indices (m) and Lags (p) for Different Models", digits = 0) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "The MV CAViaR model doesn't have an optimal value for p because there are no AR lags in the model")
    print(knitr::kable(opt_pm), format = 'pandoc')
  #   cat("\n")
  }
  # Return the xts, the plot, the loss list, and the tables
  return(list(plot_mat))
  # return(list(plot_mat, plot, l_list, tables))
}
```

```{r}
#' Function to plot the data which we generate in previous functions
#'
#' @param plot_matrix - matrix with the data to plot
#' @param norm_value - what to subtact from the data to make it on a percentage change basis. Default is 100.
#'
#' @return 
#' @export - a plot of the data by diffusion index number
#'
#' @examples = plt_data(plot_mtx[[1]]), abc = plt_data(plot_mat, tau = 0.01)
plt_data = function(plot_matrix, tau, resp_var, ntest){
  # Establish a maximum and minimum value
  max_val = max(plot_matrix[,1:ncol(plot_matrix)])
  min_val = min(plot_matrix[,1:ncol(plot_matrix)])
  # Calculate inital and ending time value
  start = index(plot_matrix)[1]
  end = index(plot_matrix)[nrow(plot_matrix)]
  ind_vals = index(plot_matrix) - start
  # Create an initial plot and add lines
    for (i in 1:ncol(plot_matrix)){
      if (i == 1){
        # 4/2/2020 - fixing the index
        plot.ts(ind_vals,plot_matrix[,i], type = "l", xlab = paste("Days Since", as.Date(start)), ylab = "Approx. Percent Change in SPY", ylim = c(min_val,max_val), lwd = 1, main = paste("Predicting", resp_var, "Returns from", as.Date(start), "to", as.Date(end)), sub = paste("The VaR Level is ", 100*tau, "%", "; There are ", ntest, " Trading Days Plotted Above", sep = ""))
        # plot.ts(index(plot_matrix), plot_matrix[,i], type = "l", xlab = "Trading Days", ylab = "Percent Change in PG", ylim = c(min_val,max_val), lwd = 1, main = "Predicting PG Returns Over Last 250 Trading Days in 2008", sub = paste("The VaR Level is ", 100*tau, "%", sep = ""))
    } else if(i %in% seq(2,8,1)) {
        lines(ind_vals,plot_matrix[,i], col = i, lty = 2)
    } else {
        lines(ind_vals,plot_matrix[,i], col = i, lty = 2)
    }
    }
  # Define a sequence for plotting
  plot_seq = seq(1, ncol(plot_matrix))
  # Modifying the code below to fix the legend on 7.31.2020
  legend("topleft", legend = c(colnames(plot_matrix)), col = plot_seq, lty = c(1, rep(2, ncol(plot_matrix)-1)), lwd = c(1, rep(1, ncol(plot_matrix)-1)))
  # Add a line for 0
  # abline(h = 0, col = "black", lty = 2)
}


```

```{r}
#' A function to calculate losses based on the test sample
#'
#' @param true_vec - the true vector of returns
#' @param pred_vec - the predicted vector from the model runs
#' @param tau - VaR level. Must match what the model used
#'
#' @return - total losses and the entire loss vector
#' @export
#'
#' @examples
loss_test = function(true_vec, pred_vec, tau){
  # Initialize a loss vector
  lvec = rep(0, length(true_vec))
  # Initialize a break vector to see when VaR is broken
  bvec = rep(0, length(true_vec))
  for (i in 1:length(true_vec)){
    # Calculate an indicator variable
    bvec[i] = ifelse(true_vec[i] < pred_vec[i], 1,0)
    # Use indicator in function below
    lvec[i] = (tau - bvec[i])*(true_vec[i] - pred_vec[i])
  }
  # Add up the losses
  # sumloss = sum(lvec)/length(lvec)
  sumloss = sum(lvec)
  # Add up the VaR breakage
  varbreak = sum(bvec)/length(bvec)
  return(list(sumloss,lvec, varbreak, bvec))
}
```



```{r}
#' A function to calculate losses based on the plot matrix
#'
#' @param data_mat - a matrix of forecasted VaR values, with the true value in the first column
#' @param tau - VaR level. Must match what the model used
#'
#' @return - a list of four items. 
#' 1 = a vector of the losses of all models. 
#' 2 = a vector showing the percentage of VaR breaks by model
#' 3 = the loss matrix
#' 4 = the break matrix
#' @export
#'
#' @examples
gen_loss_test = function(data_mat, tau){
  # Initialize loss and break matrices
  lmat = bmat = matrix(0, nrow = nrow(data_mat), ncol = ncol(data_mat)-1)
  # bvec = rep(0, length(true_vec))
  # Populate the matrices
  for (i in 1:nrow(lmat)){
    for (j in 1:(ncol(lmat))){
      # Calculate an indicator variable
      bmat[i,j] = ifelse(data_mat[i,1] < data_mat[i,j+1], 1,0)
      # Use indicator in function below
      lmat[i,j] = (tau - bmat[i,j])*(data_mat[i,1] - data_mat[i,j+1])
    }    
  }
  # Add up the losses
  sumloss = colSums(lmat)
  # Add up the VaR breakage
  varbreak = colSums(bmat)/nrow(bmat)
  return(list(sumloss, varbreak, lmat, bmat))
}
```



```{r}
#' A function to make a nice comparison of losses
#'
#' @param data_mat - input data matrix used in the calculation of losses
#' @param loss_list - a list of the losses calculated from the CAViaR function
#' @param tau - the risk level used
#' @param ntest - the number of test points
#'
#' @return
#' @export - returns a nicely formatted table
#'
#' @examples - pretty_tables(plot_mat, l_list, tau = 0.01)
pretty_tables = function(data_mat, loss_list, tau, ntest){
  # Combine into a data frame
  df = as.data.frame(rbind(loss_list[[1]], loss_list[[2]]))
  # Calculate inital and ending time value
  start = index(data_mat)[1]
  end = index(data_mat)[nrow(data_mat)]
  # Add row/column names
  colnames(df) <- colnames(data_mat[,-1])
  rownames(df) <- c("Losses", "VaR Breaks (%)")
  # Edits on 5.12.2020 - divide the table into 2
  uv_df = df[,1:4]
  mv_df = df[,5:8]
  # print(uv_df)
  # print(mv_df)
  # Convert to table
  print(uv_df %>% kable(caption = paste("Univariate CAViaR Results for a ", tau*100, "% VaR", sep = ""), digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = paste("Calculated using", ntest, "trading days from", as.Date(start), "to", as.Date(end))))
  print(mv_df %>% kable(caption = paste("Multivariate CAViaR Results for a ", tau*100, "% VaR", sep = ""), digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = paste("Calculated using", ntest, "trading days from", as.Date(start), "to", as.Date(end))))
  # Convert to a table
  # df %>% kable(caption = paste("Comparison of VaR Methods for a ", tau*100, "% VaR", sep = ""), digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = paste("Calculated using", ntest, "trading days from", as.Date(start), "to", as.Date(end)))
  # cc_df[-(1:2),] %>% kable(caption = "Accuracy of VaR Forecast for PG Over Last 200 Trading Days in 2008", digits = 3) %>% kable_styling(full_width = F) %>% footnote(general = "Tested Using the Symmetric Absolute Value Model")
}
```

# Hidden Markov Model Work

Below is arguably the most consequential plot from my M.S. thesis. The reasons for this are because it deals with an important VaR Level - 1%, which in the context of trading days means about the worst day out of 100. The biggest takeaway might be the fact that the four lines dashed lines (corresponding to the multivariate CAViaR model) do not perform as well as the last four lines, which refer to the univariate model. This can also be seen in the table of losses printed below.

## Notation

Below is the notation used later in this paper. Items 2 - 5 listed below are new multivariate CAViaR models developed in this thesis; models 6 - 9 are from the established univariate CAViaR model developed by Engle and Manganelli.

1. SPY: SPY ETF
2. No AR: Multivariate CAViaR Model with no lags
3. AR: Multivariate CAViaR Model with $p$ lags
4. SAV AR: Multivariate CAViaR Model with $p$ absolute value lags
5. AS AR: Multivariate CAViaR Model with $2p$ lags with asymmetric slopes
6. SAV: Univariate CAViaR Model with symmetric absolute framework
7. Asym. Slope: Univariate CAViaR Model with asymmetric slope framework
8. Ind. GARCH: Univariate CAViaR Model with indirect GARCH framework
9. Adaptive: Univariate CAViaR Model with adaptive  slope framework

```{r, results=  'asis'}
# Call the above function to import data from my thesis
v1_2008_alletf = var_input_disp("/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_all_ETF_runs/","var_1pc_2008_all_etf.csv", 0.01, print_graph = 1, print_mv_table = 0, print_uv_table = 0, print_opt_param =0)


```


```{r}
# These are the losses generated from our final run
test_loss = gen_loss_test(data_mat = v1_2008_alletf[[1]], tau = 0.01)[[3]]


```

```{r}
# Let's see the values of the losses
total_loss_df = as.data.frame(t(colSums(test_loss)))
rownames(total_loss_df) <- c("Losses by Model")
colnames(total_loss_df) <- colnames(v1_2008_alletf[[1]])[-1]

# Subset the data frames
first_4 <- total_loss_df[1,1:4]
last_4 <- total_loss_df[1,5:8]

# Print these out
first_4 %>% kable(caption = "Losses Over the Last 250 Trading Days in 2008 for the Multivariate CAViaR Models", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() 
last_4 %>% kable(caption = "Losses Over the Last 250 Trading Days in 2008 for the Univariate CAViaR Models", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() 
```

Based on the losses for each model during the last 250 trading days in 2008, it looks like the best options are the multivariate CAViaR model without AR terms for the multivariate model class and the symmetric absolute value model for the univariate model class. The full model specifications can be found in the appendix.

Coincidentally, these are among the simplest models available among the models plotted above. A natural criticism of this approach is that the losses are lower for the CAViaR specifications without lagged predictors. This is a fair point, however, the period of 2008 is a period of extreme crisis, and a simpler, ARMA-style model might seem to work better other things equal. Future work can extend this work to other periods. Since these are the best two options during this period of interest, the next step is to find reasonable parametric distributions to model their values.

## Finding Distributions of the Forecasts

To fit the Hidden Markov Model, it is necessary to find distributions that appropriately fit the two models above. While the distribution of predictions from the multivariate model is fairly well-approximated by a normal distribution, the distribution of predictions from the univariate model was highly left-skewed, which makes fitting a distribution difficult.

The first step was to try transformations of the aforementioned predictions from the univariate model, namely $\sqrt{\max(x+1) -x}$, $\log_{10} (\max(x+1) -x)$, and $1/(\max(x+1) -x)$ following the suggestions found here [@Kassambara]. While these did (in some cases) eliminate the left-skew, it often created a more pronounced right-skew! Thus, I attempted to model the data without transformation.

There are many candidate distributions that could be used to model the empirical distribution of values from the univariate CAViaR model, but the three that stood out are:

1. The Skew Normal Distribution (parameterized by location, scale, and shape parameters)
2. The Gumbel Distribution (parameterized by location and scale parameters)
3. The Generalized Extreme Value Distribution (parameterized by location, scale, and shape parameters)



```{r}
# Extract the data
small_df = v1_2008_alletf[[1]][,c(1,2,6)]

# Add 1
# small_df2 = small_df + 1

# These fits don't work very well
# hist(log(small_df2[,2]), breaks = 25)
# hist(small_df2[,3], breaks = 25)
# hist(log(small_df2[,3]), breaks = 25)
# hist((small_df[,3]), breaks = 25)

# small_df2
# ?snormFit

# Let's try another transformation
# t1 = sqrt(max(small_df[,3]+1) - small_df[,3])
# t2 = log10(max(small_df[,3]+1) - small_df[,3])
# t3 = 1/(max(small_df[,3]+1) - small_df[,3])

# Plot the transformations
# hist(t1, breaks = 25)
# hist(t2, breaks = 25)
# hist(t3, breaks = 25)


```


```{r}
# Fit a skew normal
m1 = snormFit(small_df[,2])
m2 = snormFit(small_df[,3])

# m1

# snormFit(small_df[,2]+1)
# snormFit(small_df[,3]+1)
# These may not work very well

# square_val = small_df[,3]^2
# m3 = snormFit(square_val)
# m4 = snormFit(t3)
# m4



# hist((small_df[,3])^2, breaks = 25)
```


```{r}
# Fit a gumbel function
gum_param = gum.fit(small_df[,3], show = FALSE)
# ?gum.fit
# gum_param$mle

# Try fitting a GEV distribution
gev_param = gev.fit(small_df[,3], show = FALSE)
# gev_param$mle

# # Plot histograms
# # Gumbel
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgumbel(xfit, loc = gum_param$mle[1], scale = gum_param$mle[2])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# # GEV
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)

par(mfrow = c(1,2))

# Let's look at the data
h = hist(small_df[,2], breaks = 25, xlab = "Modeled Values", main = "Multivariate CAViaR")
# Overset a density function
xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
yfit = dsnorm(xfit, mean = m1$par[1], sd = m1$par[2], xi = m1$par[3])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,2])
lines(xfit, yfit, col="blue", lwd=2)
# Overset a normal density function
xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
yfit_n = dnorm(xfit, mean = mean(small_df[,2]), sd = sd(small_df[,2]))
yfit_n <- yfit_n*diff(h$mids[1:2])*length(small_df[,2])
lines(xfit, yfit_n, col="red", lwd=2, lty = 2)

# Add a legend
legend("topleft", legend = c("Normal", "Skew Normal"), col = c("blue", "red"), lty = c(1,2), lwd = c(2,2))



# Let's look at the second set of data
h = hist(small_df[,3], breaks = 25, xlab = "Modeled Values", main = "Univariate CAViaR")
# Overset a density function - Skew Normal
xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
yfit = dsnorm(xfit, mean = m2$par[1], sd = m2$par[2], xi = m2$par[3])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
lines(xfit, yfit, col="blue", lwd=2)
# Overset a density function - Gumbel
xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
yfit = dgumbel(xfit, loc = gum_param$mle[1], scale = gum_param$mle[2])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
lines(xfit, yfit, col="red", lwd=2)
# Overset a density function - GEV
xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
lines(xfit, yfit, col="green", lwd=2)

# Add a legend
legend("topleft", legend = c("Skew Normal", "Gumbel", "GEV"), col = c("blue", "red", "green"), lty = c(1,1,1), lwd = c(2,2,2))

# par(mfrow = c(1,3))
# 
# # Let's look at the data
# h = hist(small_df[,2], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
# yfit = dsnorm(xfit, mean = m1$par[1], sd = m1$par[2], xi = m1$par[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,2])
# lines(xfit, yfit, col="blue", lwd=2)
# # Overset a normal density function
# xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
# yfit_n = dnorm(xfit, mean = mean(small_df[,2]), sd = sd(small_df[,2]))
# yfit_n <- yfit_n*diff(h$mids[1:2])*length(small_df[,2])
# lines(xfit, yfit_n, col="red", lwd=2, lty = 2)
# 
# # Let's look at the second set of data
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dsnorm(xfit, mean = m2$par[1], sd = m2$par[2], xi = m2$par[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# # # Let's look at the second set of data
# # h = hist(square_val, breaks = 25)
# # # Overset a density function
# # xfit<-seq(min(square_val),max(square_val),length=250)
# # yfit = dsnorm(xfit, mean = m3$par[1], sd = m3$par[2], xi = m3$par[3])
# # yfit <- yfit*diff(h$mids[1:2])*length(square_val)
# # lines(xfit, yfit, col="blue", lwd=2)
# 
# # Let's look at the second set of data
# h = hist(t3, breaks = 25)
# # Overset a density function
# xfit<-seq(min(t3),max(t3),length=250)
# yfit = dsnorm(xfit, mean = m4$par[1], sd = m4$par[2], xi = m4$par[3])
# yfit <- yfit*diff(h$mids[1:2])*length(t3)
# lines(xfit, yfit, col="blue", lwd=2)

```



```{r}
# # Fit a gumbel function
# gum_param = gum.fit(small_df[,3])
# gum_param$mle
# 
# # Try fitting a GEV distribution
# gev_param = gev.fit(small_df[,3])
# gev_param$mle
# 
# # Plot histograms
# # Gumbel
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgumbel(xfit, loc = gum_param$mle[1], scale = gum_param$mle[2])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# # GEV
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)

```

To fit the above histograms,the optimal parameters were fit using maximum likelihood. Note that while there were convergence issues used in fitting the skew-normal distribution, there were not issues with fitting the Gumbel or the GEV distributions.

```{r}
# Below are the optimal parameters for the second model
norm_param_df = as.data.frame(cbind(mean(small_df[,2]),sd(small_df[,2])))
rownames(norm_param_df) <- c("Optimal Parameters")
colnames(norm_param_df) <- c("Mean", "Standard Deviation")

# Display a pretty table
norm_param_df %>% kable(caption = "Optimal Parameters for the Normal Distribution", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "The Mean Was Estimated Using the Sample Mean, SD was Estimated Using Sample Std. Dev.")

# Rewrite m2 as the optimal parameters of skew normal
sn_params = m2$par

# Display the optimal skew-normal parameters
sn_param_df = as.matrix(t(sn_params))
# sn_param_df
# rownames(sn_param_df) <- c("Optimal Parameters")
# colnames(sn_param_df) <- c("Location", "Scale", "Shape")

# Display a pretty table
# sn_param_df %>% kable(caption = "Optimal Parameters for the Skew Normal Distribution", digits = 4) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")

# gum_param$mle

# Display the optimal Gumbel parameters
gum_param_df = as.matrix(t(c(gum_param$mle,0)))
# gum_param_df
# rownames(gum_param_df) <- c("Optimal Parameters")
# colnames(gum_param_df) <- c("Location", "Scale")

# Display a pretty table
# gum_param_df %>% kable(caption = "Optimal Parameters for the Gumbel Distribution", digits = 4) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")


# Take out the optimal parameters
gev_param_df = as.matrix(t(gev_param$mle))
# rownames(gev_param_df) <- c("Optimal Parameters")
# colnames(gev_param_df) <- c("Location", "Scale", "Shape")

# Display a pretty table
# gev_param_df %>% kable(caption = "Optimal Parameters for the Generalized Extreme Value Distribution", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")

# Combine all of the extreme value distributions together
ev_param_df = as.data.frame(rbind(sn_param_df, gum_param_df, gev_param_df))
rownames(ev_param_df) <- c("Skew Normal", "Gumbel", "GEV")
colnames(ev_param_df) <- c("Location", "Scale", "Shape")

# Add a missing value for Gumbel shape
ev_param_df[2,3] <- NA

# Display a pretty table
ev_param_df %>% kable(caption = "Optimal Parameters for the Candidate Distributions", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")


```

To evaluate the model fit more rigorously, I compared the Kullback-Leibler divergence for each theoretical distribution.

```{r}
# First, find the density of the data
m3_den = hist(small_df[,3], breaks = 25, freq = FALSE, plot = FALSE)
# ?hist
# m3_den$density

# Rewrite m2 as the optimal parameters of skew normal
# sn_params = m2$par
# sn_params

# Next compute the probability at each point for the fitted models
## Skew-normal
skn_den = dsnorm(m3_den$mids, mean = m2$par[1], sd = m2$par[2], xi = m2$par[3])
## Gumbel
gumb_den = dgumbel(m3_den$mids, loc = gum_param$mle[1], scale = gum_param$mle[2])
## GEV
gev_den = dgev(m3_den$mids, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])

# Calculate the K-L divergences for each
kl_skn = KLD(m3_den$density, skn_den)
kl_gumb = KLD(m3_den$density, gumb_den)
kl_gev = KLD(m3_den$density, gev_den)

# Combine into a data frame
kl_df = as.data.frame(cbind(kl_skn$mean.sum.KLD, kl_gumb$mean.sum.KLD, kl_gev$mean.sum.KLD))

rownames(kl_df) <- c("Mean Sum K-L Divergence")
colnames(kl_df) <- c("Skew-Normal", "Gumbel", "GEV")

# Display a pretty table
kl_df %>% kable(caption = "Comparing K-L Divergences By Model Fits", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() 
```

Based on the K-L Divergence, it would seem to make sense to use the generalized extreme value distribution, however, the problem with doing this is the fact that this distribution doesn't have support over the entire real line. Moreover, the Skew-Normal distribution did not work well when fitting the Hidden Markov Model. Therefore, the Gumbel is used in fitting the HMM below.

## HMM Background and Results

The motivating idea behind a Hidden Markov Model is that there are 2 unknown latent states $k$ that generate the data that is seen. (The reference for this information is given here [@Stephens2018]). The algorithm implemented here computes forwards probabilities, $\alpha_{tk} = \mathbb{P}(X_1, ... , X_t; Z_t = k)$. To start, one simply multiplies an equally-weighted prior $\pi_k = 0.5$ by the likelihood of the data given each state, given by $\mathbb{P}(X_1 | Z_1 = k$). 

The likelihood function for the "rational" state (represented by the multivariate CAViaR model) is represented by a normal distribution whereas the likelihood function for the "irrational" state (represented by the univariate CAViaR model) is represented by the Gumbel distribution. Both use the parameters estimated above.

Now, once the $\alpha_1$ value is calculate, $\alpha_2$ is calculated as follows, with a similar process for an arbitrary value $\alpha_t$.

$$
\alpha_2 = (\alpha_1 P)_k \mathbb{P}(X_2 | Z_2 = k)
$$

The $P$ symbol corresponds to a symmetric 2x2 transition matrix where the first row is $(0.9, 0.1)$ and the second row is $(0.1, 0.9)$.


To compute the backwards probabilities, we compute the following $\beta_{tk} = \mathbb{P}(X_{t+1}, ... , X_T; Z_t = k)$, and then the posterior distribution for each state $Z_t$ is given by the following:

$$
\mathbb{P}(Z_t = k| X_1, ..., X_T) = \alpha_{tk} \beta_{tk} / \sum_{k}\alpha_{tk} \beta_{tk}
$$

```{r}
# # Below are the optimal parameters for the second model
# norm_param_df = as.data.frame(cbind(mean(small_df[,2]),sd(small_df[,2])))
# rownames(norm_param_df) <- c("Optimal Parameters")
# colnames(norm_param_df) <- c("Mean", "Standard Deviation")
# 
# # Display a pretty table
# norm_param_df %>% kable(caption = "Optimal Parameters for the Normal Distribution", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "The Mean Was Estimated Using the Sample Mean, SD was Estimated Using Sample Std. Dev.")
# 
# # sn_params
# 
# # Display the optimal skew-normal parameters
# sn_param_df = as.data.frame(t(sn_params))
# rownames(sn_param_df) <- c("Optimal Parameters")
# colnames(sn_param_df) <- c("Location", "Scale", "Shape")
# 
# # Display a pretty table
# sn_param_df %>% kable(caption = "Optimal Parameters for the Skew Normal Distribution", digits = 4) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")
# 
# # gum_param$mle
# 
# # Display the optimal Gumbel parameters
# gum_param_df = as.data.frame(t(gum_param$mle))
# rownames(gum_param_df) <- c("Optimal Parameters")
# colnames(gum_param_df) <- c("Location", "Scale")
# 
# # Display a pretty table
# gum_param_df %>% kable(caption = "Optimal Parameters for the Gumbel Distribution", digits = 4) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")
# 
# 
# # Take out the optimal parameters
# gev_param_df = as.data.frame(t(gev_param$mle))
# rownames(gev_param_df) <- c("Optimal Parameters")
# colnames(gev_param_df) <- c("Location", "Scale", "Shape")
# 
# # Display a pretty table
# gev_param_df %>% kable(caption = "Optimal Parameters for the Generalized Extreme Value Distribution", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")
```




```{r}
# Clean out old variables
# rm(list=ls())

# This is the true model used in the simulation study. We won't need it.
# set.seed(1)
# T = 200
# K = 2
# sd= 0.4
# P = cbind(c(0.9,0.1),c(0.1,0.9))

# Simulate the latent (Hidden) Markov states
# Also not needed.
# Z = rep(0,T)
# Z[1] = 1
# for(t in 1:(T-1)){
#   Z[t+1] = sample(K, size=1, prob=P[Z[t],])
# }


# Simulate the emitted/observed values
# X= rnorm(T,mean=Z,sd=sd)
# X

# plot(X, main="Realization of HMM; latent states shown in red")
# lines(Z,col=2,lwd=2)

# Instead, let's subset our data frame
# small_test = test_loss[,c(1,5)]
# colSums(small_test)

# Set a value of T and K
T = nrow(small_df)
K = ncol(small_df) - 1

# small_df

# Assign a value of P
P = cbind(c(0.9,0.1),c(0.1,0.9))

# Plot these data
# ?hist
# par(mfrow= c(1,2))
# hist(small_test[,1], breaks = 25)
# hist(small_test[,2], breaks = 25)
# 
# # Transform the dataset
# log_test = log(small_test)
# 
# # Plot the logs?
# par(mfrow= c(1,2))
# h= hist(log_test[,1], breaks = 25)
# # Overset a density function
# xfit<-seq(min(log_test[,1]),max(log_test[,1]),length=40)
# yfit<-dnorm(xfit,mean=mean(log_test[,1]),sd=sd(log_test[,1]))
# yfit <- yfit*diff(h$mids[1:2])*length(log_test[,1])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# # Repeat the same thing
# h= hist(log_test[,2], breaks = 25)
# # Overset a density function
# xfit<-seq(min(log_test[,2]),max(log_test[,2]),length=40)
# yfit<-dnorm(xfit,mean=mean(log_test[,2]),sd=sd(log_test[,2]))
# yfit <- yfit*diff(h$mids[1:2])*length(log_test[,2])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# 
# # Find the means and SDs
# v1 = rbind(mean(log_test[,1]), sd(log_test[,1]))
# 
# # Find the means and SDs
# v2 = rbind(mean(log_test[,2]), sd(log_test[,2]))
# 
# # Store as a data frame
# sum_df = as.data.frame(cbind(v1,v2))
# rownames(sum_df) <- c("Mean", "SD")
# colnames(sum_df) <- c("No AR", "SAV")
# 
# # Format nicely
# sum_df %>% kable(caption = "Summary Statistics for the Log of Losses from Each Model", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling()

# log_test


```




```{r}
# this is the function Pr(X_t | Z_t=k) for our example
# This is the emission probability in the "rational world", modified for whether we miss positive or negative
emit_norm = function(obs,mean, sd){ 
  dnorm(obs, mean = mean, sd = sd)
  # dnorm(x,mean=k,sd=sd)
}

# Here is where we will input our emission probabilities for a GEV
emit_gev = function(obs,loc, scale, shape){ 
  dgev(obs, loc = loc, scale = scale, shape = shape)
}

# The GEV doesn't have support over all of the real line, which is a problem
emit_sn = function(obs,loc, scale, shape){ 
  dsnorm(obs, mean = loc, sd = scale, xi = shape)
}

# Let's try the Gumbel distribution
emit_gumb = function(obs,loc, scale){ 
  dgumbel(obs, loc = loc, scale = scale)
}

# sn_param_df

# Our prior is that both states are equally likely.
prior = c(0.5,0.5) #Assumed prior distribution on Z_1

# The matrix where we store the forwards probabilities
alpha = matrix(nrow = T,ncol=K)


# for(k in 1:K){ 
#   alpha[1,k] = prior[k] * emit(k,X[1])
# }

# head(small_df)
# norm_param_df$`Standard Deviation`
# gev_param_df$Location

# sn_param_df$Location
gum_param_df = ev_param_df[2,1:2]
# gum_param_df$Location

# Initialize alpha[1,]
for(k in 1:K){
  if (k == 1){
    # This is the emission probability assuming normality
    alpha[1,k] = prior[k] * emit_norm(obs = small_df$SPY[1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
  } else if (k == 2){
    # This is the emission probability assuming GEV
    # alpha[1,k] = prior[k] * emit_gev(obs = small_df$SPY[1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
    # This is the emission probability assuming SN
    # alpha[1,k] = prior[k] * emit_sn(obs = small_df$SPY[1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
    # Third time is the charm...assume a Gumbel
    alpha[1,k] = prior[k] * emit_gumb(obs = small_df$SPY[1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
  } else {
    print("k should not be greater than 2")
  }
}

# head(alpha)
# small_df$SPY[1]
# alpha[1,] %*% P


# Forward algorithm
for(t in 1:(T-1)){
  # Find the value of m at each step
  m = alpha[t,] %*% P
  # Loop through to update levels of alpha
  for(k in 1:K){
    # alpha[t+1,k] = m[k]*emit(k,X[t]) what I had
    # alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
    if (k == 1){
      # alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
      alpha[t+1,k] = m[k]*emit_norm(obs = small_df$SPY[t+1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
    } else if (k == 2){
      # Assuming a GEV
      # alpha[t+1,k] = m[k]*emit_gev(obs = small_df$SPY[t+1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
      # Assuming a skew-normal
      # alpha[t+1,k] = m[k]*emit_sn(obs = small_df$SPY[t+1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
      # Assuming a Gumbel
      alpha[t+1,k] = m[k]*emit_gumb(obs = small_df$SPY[t+1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
    } else {
      print("k should not be greater than 2")
    }
    # alpha[t+1,k] = m[k]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k])
  }
}

# emit_gev(obs = small_df$SPY[2], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
# emit_sn(obs = small_df$SPY[2], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
# sn_param_df$Location
# sn_param_df$Scale
# sn_param_df$Shape

# dsnorm(small_df$SPY[2], mean = sn_param_df$Location, sd = sn_param_df$Scale, xi = sn_param_df$Shape)
# hist(rsnorm(10000, mean = sn_param_df$Location, sd = sn_param_df$Scale, xi = sn_param_df$Shape), xlim = c(-2,0))

# Try it with a gumbel
# dgumbel(small_df$SPY[3], loc = gum_param_df$Location, scale = gum_param_df$Scale)

# small_df$SPY[2]

# alpha

```


```{r}
# Initalize a beta matrix
beta = matrix(nrow = T,ncol=K)

# Initialize beta
for(k in 1:K){
  beta[T,k] = 1
}

# Backwards algorithm
for(t in (T-1):1){
  for(k in 1:K){
    # Modify the value of beta
    # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
    # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k]))
    # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
    if (k == 1){
      # Here's what was online
      # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
      # Find the values separately
      v1 = emit_norm(obs = small_df$SPY[t+1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
      v2 = emit_gumb(obs = small_df$SPY[t+1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
      beta[t,k] = sum(beta[t+1,]*P[k,]*c(v1, v2))
    } else if (k == 2){
      # Here's what was online
      # Find the values separately
      v1 = emit_norm(obs = small_df$SPY[t+1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
      v2 = emit_gumb(obs = small_df$SPY[t+1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
      beta[t,k] = sum(beta[t+1,]*P[k,]*c(v1, v2))
    } else {
      print("k should not be greater than 2")
    }
  }
}




```


```{r}
# Lastly, we need to solve for the posterior
ab = alpha*beta
prob = ab/rowSums(ab)
# rowSums(prob)
# prob

plot(prob[,2],type="l",ylim=c(0,1), main="Chance the World is in an Irrational State",lwd=2,ylab="Posterior Probability")
# lines(Z==2,col=2,lwd=2)
```

```{r}

# Bolt on an NA
norm_param_df2 = cbind(norm_param_df, NA)
# Rename rows and columns
rownames(norm_param_df2) <- c("Normal")
colnames(norm_param_df2) <- c("Location", "Scale", "Shape")

# Combine into an optimal parameter df
opt_param_df = rbind(norm_param_df2, ev_param_df)
opt_param_df$Location
opt_param_df

abc = opt_param_df[c("Normal"),]
abc$Location
```


```{r}
#' A function to fit multiple HMMs given an input dataset
#'
#' @param input_df - input dataframe
#' @param t_prob - transition probability
#' @param prior_prob - prior probability
#' @param irr_dist - the irrational distribution. Right now, this code is only set up to work with Gumbel
#' @param rat_dist - rational distribution. Right now, only designed for normal.
#'
#' @return
#' @export
#'
#' @examples
hmm_fit = function(input_df, t_prob, prior_prob, irr_dist = "Gumbel", rat_dist = "Normal"){
  # Set a value of T and K
  T = nrow(input_df)
  K = ncol(input_df) - 1
  # Initialize optimal parameters for rational and irrational datasets
  rat_opt_param = opt_param_df[c(rat_dist),]
  irr_opt_param = opt_param_df[c(irr_dist),]
  print(rat_opt_param)
  # Encode the prior
  prior = c(prior_prob, 1- prior_prob) #Assumed prior distribution on Z_1
  # Assign a value of P - the transition matrix
  P = cbind(c(1-t_prob,t_prob),c(t_prob,1-t_prob))
  # Initialize alpha[1,]
  for(k in 1:K){
    if (k == 1){
      # This is the emission probability in the "rational" world
      alpha[1,k] = prior[k] * emit_norm(obs = input_df$SPY[1], mean = rat_opt_param$Location, sd = rat_opt_param$Scale)
    } else if (k == 2){
      # This is the emission probability assuming GEV
      # alpha[1,k] = prior[k] * emit_gev(obs = small_df$SPY[1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
      # This is the emission probability assuming SN
      # alpha[1,k] = prior[k] * emit_sn(obs = small_df$SPY[1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
      # Third time is the charm...assume a Gumbel
      alpha[1,k] = prior[k] * emit_gumb(obs = input_df$SPY[1], loc = irr_opt_param$Location, scale = irr_opt_param$Scale)
    } else {
      print("k should not be greater than 2")
    }
  }
  # Forward algorithm
  for(t in 1:(T-1)){
    # Find the value of m at each step
    m = alpha[t,] %*% P
    # Loop through to update levels of alpha
    for(k in 1:K){
      # alpha[t+1,k] = m[k]*emit(k,X[t]) what I had
      # alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
      if (k == 1){
        # alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
        alpha[t+1,k] = m[k]*emit_norm(obs = input_df$SPY[t+1], mean = rat_opt_param$Location, sd = rat_opt_param$Scale)
      } else if (k == 2){
        # Assuming a GEV
        # alpha[t+1,k] = m[k]*emit_gev(obs = small_df$SPY[t+1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
        # Assuming a skew-normal
        # alpha[t+1,k] = m[k]*emit_sn(obs = small_df$SPY[t+1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
        # Assuming a Gumbel
        alpha[t+1,k] = m[k]*emit_gumb(obs = input_df$SPY[t+1], loc = irr_opt_param$Location, scale = irr_opt_param$Scale)
      } else {
        print("k should not be greater than 2")
      }
      # alpha[t+1,k] = m[k]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k])
    }
  }
  # Initalize a beta matrix
  beta = matrix(nrow = T,ncol=K)
  # Initialize beta
  for(k in 1:K){
    beta[T,k] = 1
  }
  # Backwards algorithm
  for(t in (T-1):1){
    for(k in 1:K){
      # Modify the value of beta
      # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
      # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k]))
      # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
      if (k == 1){
        # Here's what was online
        # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
        # Find the values separately
        v1 = emit_norm(obs = input_df$SPY[t+1], mean = rat_opt_param$Location, sd = rat_opt_param$Scale)
        v2 = emit_gumb(obs = input_df$SPY[t+1], loc = irr_opt_param$Location, scale = irr_opt_param$Scale)
        beta[t,k] = sum(beta[t+1,]*P[k,]*c(v1, v2))
      } else if (k == 2){
        # Here's what was online
        # Find the values separately
        v1 = emit_norm(obs = input_df$SPY[t+1],  mean = rat_opt_param$Location, sd = rat_opt_param$Scale)
        v2 = emit_gumb(obs = input_df$SPY[t+1],  loc = irr_opt_param$Location, scale = irr_opt_param$Scale)
        beta[t,k] = sum(beta[t+1,]*P[k,]*c(v1, v2))
      } else {
        print("k should not be greater than 2")
      }
    }
  }
  # Lastly, we need to solve for the posterior
  ab = alpha*beta
  prob = ab/rowSums(ab)
  # Plot the Data  
  plot(prob[,2],type="l",ylim=c(0,1), main="Chance the World is in an Irrational State",lwd=2,ylab="Posterior Probability")
  # lines(Z==2,col=2,lwd=2)
}
```

```{r}
hmm_fit(input_df = small_df, t_prob = 0.1, prior_prob = 0.5, irr_dist = "Gumbel")
```

# FUNCTIONS


```{r, results=  'asis'}
# Call the above function to import data from my thesis
v1_2008_alletf1 = var_input_disp("/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_all_ETF_runs/","var_1pc_2008_all_etf.csv", 0.01, print_graph = 0, print_mv_table = 0, print_uv_table = 0, print_opt_param =0)
v1_2008_alletf2 = var_input_disp("/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_all_ETF_runs/","var_1pc_2008_all_etf.csv", 0.05, print_graph = 0, print_mv_table = 0, print_uv_table = 0, print_opt_param =0)
```



```{r}
#' This is a function to read in the losses from previous years
#'
#' @param var_level - the VaR level as an integer
#' @param year - the year to read in. 2008, 2010, 2014, and 2016.
#' @param path - the path to the files. You probably shouldn't change this.
#'
#' @return - the losses by year
#' @export
#'
#' @examples - import_dat(1, 2008)
import_dat = function(var_level, year, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_all_ETF_runs/"){
  # Import the data frame
  int_df = var_input_disp(path, paste0("var_", var_level, "pc_", year, "_all_etf.csv"), 0.01, print_graph = 0, print_mv_table = 0, print_uv_table = 0, print_opt_param =0)
  # Extract the losses
  losses = gen_loss_test(data_mat = int_df[[1]], tau = var_level/100)[[3]]
  # Assign the predictions
  preds = int_df[[1]]
  # Return the losses
  return(list(preds = preds, losses = losses))
}

# v1_2008_alletf

dat1 = import_dat(1, 2008)
# abc
# abcde =colSums(abc)
# abcde
# 
# which.min(abcde[1:8])
```


```{r}
#' A function that chooses the optimal models
#'
#' @param loss_mat - loss matrix from previous code
#'
#' @return - the optimal models for the HMM
#' @export
#'
#' @examples - pick_best(dat1)
pick_best = function(loss_mat){
  # Find the column sums
  cs = colSums(loss_mat)
  # Subset into two - rational and irrational parts (split in two)
  rat_sum = cs[1:4]
  irr_sum = cs[5:8]
  # Find the minimizer of each set
  rat_min = which.min(rat_sum)
  # Add 4 for the offset
  irr_min = which.min(irr_sum) + 4
  # print(c(rat_min, irr_min))
  # Return the optimal pairs
  return(c(rat_min, irr_min))
}

best = pick_best(dat1$losses)

c(1,best+1)
# These are the losses generated from our final run
# test_loss = gen_loss_test(data_mat = v1_2008_alletf[[1]], tau = 0.01)[[3]]


```

```{r}
# Subset the data

#' A function to subset the data and print the optimal models
#'
#' @param opt_params - the optimal parameters from the pick_best function
#' @param init_df - the initial data frame to read off of
#' @param print_best_mods - print the best models that will go through the optimization process
#'
#' @return - a subsetted data frame
#' @export
#'
#' @examples - test_df = small_df_fn(best, dat1$preds, print_best_mods = 1)
small_df_fn = function(opt_params, init_df, print_best_mods = 0){
  # Select the SP500 and the optimal columsn from earlier
  small_df= init_df[,c(1,opt_params+1)]
  # Print the optimal parameters
  if (print_best_mods != 0){
    print(paste0("The best multivariate CAViaR model for the time period from ",
                  min(index(small_df)), " to ", max(index(small_df))," is ", colnames(small_df)[2], 
                 " and the best univariate CAViaR model for the time period is ", colnames(small_df)[3]))
  }
  # Return the data frame
  return(small_df)
}

small_df1 = small_df_fn(best, dat1$preds, print_best_mods = 1)


# test_df = small_df_fn(best, dat1$preds, print_best_mods = 1)




```

```{r}
# Fit models for the univariate CAViaR models
uv_cav_fit = function(input_df){
  # Fit skew normal
  sn_param = snormFit(input_df[,3])
  # Fit Gumbel
  gum_param = gum.fit(input_df[,3], show = FALSE)
  # Plot the data
  h = hist(input_df[,3], breaks = 25, xlab = "Modeled Values", main = "Univariate CAViaR")
  # Overset a density function - Skew Normal
  xfit<-seq(min(input_df[,3]),max(input_df[,3]),length=250)
  yfit = dsnorm(xfit, mean = sn_param$par[1], sd = sn_param$par[2], xi = sn_param$par[3])
  yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
  lines(xfit, yfit, col="blue", lwd=2)
  # Overset a density function - Gumbel
  xfit<-seq(min(input_df[,3]),max(input_df[,3]),length=250)
  yfit = dgumbel(xfit, loc = gum_param$mle[1], scale = gum_param$mle[2])
  yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
  lines(xfit, yfit, col="red", lwd=2)
  # Overset a density function - GEV
  # xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
  # yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
  # yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
  # lines(xfit, yfit, col="green", lwd=2)
  # Add a legend
  legend("topleft", legend = c("Skew Normal", "Gumbel"), col = c("blue", "red"), lty = c(1,1,1), lwd = c(2,2,2))
  
  # Combine all of the extreme value distributions together
  ev_param_df = as.data.frame(rbind(sn_param_df, gum_param_df, gev_param_df))
  rownames(ev_param_df) <- c("Skew Normal", "Gumbel", "GEV")
  colnames(ev_param_df) <- c("Location", "Scale", "Shape")
  # Rewrite m2 as the optimal parameters of skew normal
  sn_params = m2$par
  
  # Display the optimal skew-normal parameters
  sn_param_df = as.matrix(t(sn_params))
  # sn_param_df
  # rownames(sn_param_df) <- c("Optimal Parameters")
  # colnames(sn_param_df) <- c("Location", "Scale", "Shape")
  
  # Display a pretty table
  # sn_param_df %>% kable(caption = "Optimal Parameters for the Skew Normal Distribution", digits = 4) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")
  
  # gum_param$mle
  
  # Display the optimal Gumbel parameters
  gum_param_df = as.matrix(t(c(gum_param$mle,0)))
  
  # Add a missing value for Gumbel shape
  ev_param_df[2,3] <- NA
  
  # Display a pretty table
  ev_param_df %>% kable(caption = "Optimal Parameters for the Candidate Distributions", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")
}

uv_cav_fit(small_df1)


```


```{r}
# Fit a skew normal
m1 = snormFit(small_df[,2])
m2 = snormFit(small_df[,3])

# m1

# snormFit(small_df[,2]+1)
# snormFit(small_df[,3]+1)
# These may not work very well

# square_val = small_df[,3]^2
# m3 = snormFit(square_val)
# m4 = snormFit(t3)
# m4



# hist((small_df[,3])^2, breaks = 25)
```


```{r}
# Fit a gumbel function
gum_param = gum.fit(small_df[,3], show = FALSE)
# ?gum.fit
# gum_param$mle

# Try fitting a GEV distribution
gev_param = gev.fit(small_df[,3], show = FALSE)
# gev_param$mle

# # Plot histograms
# # Gumbel
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgumbel(xfit, loc = gum_param$mle[1], scale = gum_param$mle[2])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# # GEV
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)

par(mfrow = c(1,2))

# Let's look at the data
h = hist(small_df[,2], breaks = 25, xlab = "Modeled Values", main = "Multivariate CAViaR")
# Overset a density function
xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
yfit = dsnorm(xfit, mean = m1$par[1], sd = m1$par[2], xi = m1$par[3])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,2])
lines(xfit, yfit, col="blue", lwd=2)
# Overset a normal density function
xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
yfit_n = dnorm(xfit, mean = mean(small_df[,2]), sd = sd(small_df[,2]))
yfit_n <- yfit_n*diff(h$mids[1:2])*length(small_df[,2])
lines(xfit, yfit_n, col="red", lwd=2, lty = 2)

# Add a legend
legend("topleft", legend = c("Normal", "Skew Normal"), col = c("blue", "red"), lty = c(1,2), lwd = c(2,2))



# Let's look at the second set of data
h = hist(small_df[,3], breaks = 25, xlab = "Modeled Values", main = "Univariate CAViaR")
# Overset a density function - Skew Normal
xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
yfit = dsnorm(xfit, mean = m2$par[1], sd = m2$par[2], xi = m2$par[3])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
lines(xfit, yfit, col="blue", lwd=2)
# Overset a density function - Gumbel
xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
yfit = dgumbel(xfit, loc = gum_param$mle[1], scale = gum_param$mle[2])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
lines(xfit, yfit, col="red", lwd=2)
# Overset a density function - GEV
xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
lines(xfit, yfit, col="green", lwd=2)

# Add a legend
legend("topleft", legend = c("Skew Normal", "Gumbel", "GEV"), col = c("blue", "red", "green"), lty = c(1,1,1), lwd = c(2,2,2))

# par(mfrow = c(1,3))
# 
# # Let's look at the data
# h = hist(small_df[,2], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
# yfit = dsnorm(xfit, mean = m1$par[1], sd = m1$par[2], xi = m1$par[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,2])
# lines(xfit, yfit, col="blue", lwd=2)
# # Overset a normal density function
# xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
# yfit_n = dnorm(xfit, mean = mean(small_df[,2]), sd = sd(small_df[,2]))
# yfit_n <- yfit_n*diff(h$mids[1:2])*length(small_df[,2])
# lines(xfit, yfit_n, col="red", lwd=2, lty = 2)
# 
# # Let's look at the second set of data
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dsnorm(xfit, mean = m2$par[1], sd = m2$par[2], xi = m2$par[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# # # Let's look at the second set of data
# # h = hist(square_val, breaks = 25)
# # # Overset a density function
# # xfit<-seq(min(square_val),max(square_val),length=250)
# # yfit = dsnorm(xfit, mean = m3$par[1], sd = m3$par[2], xi = m3$par[3])
# # yfit <- yfit*diff(h$mids[1:2])*length(square_val)
# # lines(xfit, yfit, col="blue", lwd=2)
# 
# # Let's look at the second set of data
# h = hist(t3, breaks = 25)
# # Overset a density function
# xfit<-seq(min(t3),max(t3),length=250)
# yfit = dsnorm(xfit, mean = m4$par[1], sd = m4$par[2], xi = m4$par[3])
# yfit <- yfit*diff(h$mids[1:2])*length(t3)
# lines(xfit, yfit, col="blue", lwd=2)

```



```{r}
# # Fit a gumbel function
# gum_param = gum.fit(small_df[,3])
# gum_param$mle
# 
# # Try fitting a GEV distribution
# gev_param = gev.fit(small_df[,3])
# gev_param$mle
# 
# # Plot histograms
# # Gumbel
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgumbel(xfit, loc = gum_param$mle[1], scale = gum_param$mle[2])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# # GEV
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)

```

To fit the above histograms,the optimal parameters were fit using maximum likelihood. Note that while there were convergence issues used in fitting the skew-normal distribution, there were not issues with fitting the Gumbel or the GEV distributions.

```{r}
# Below are the optimal parameters for the second model
norm_param_df = as.data.frame(cbind(mean(small_df[,2]),sd(small_df[,2])))
rownames(norm_param_df) <- c("Optimal Parameters")
colnames(norm_param_df) <- c("Mean", "Standard Deviation")

# Display a pretty table
norm_param_df %>% kable(caption = "Optimal Parameters for the Normal Distribution", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "The Mean Was Estimated Using the Sample Mean, SD was Estimated Using Sample Std. Dev.")

# Rewrite m2 as the optimal parameters of skew normal
sn_params = m2$par

# Display the optimal skew-normal parameters
sn_param_df = as.matrix(t(sn_params))
# sn_param_df
# rownames(sn_param_df) <- c("Optimal Parameters")
# colnames(sn_param_df) <- c("Location", "Scale", "Shape")

# Display a pretty table
# sn_param_df %>% kable(caption = "Optimal Parameters for the Skew Normal Distribution", digits = 4) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")

# gum_param$mle

# Display the optimal Gumbel parameters
gum_param_df = as.matrix(t(c(gum_param$mle,0)))
# gum_param_df
# rownames(gum_param_df) <- c("Optimal Parameters")
# colnames(gum_param_df) <- c("Location", "Scale")

# Display a pretty table
# gum_param_df %>% kable(caption = "Optimal Parameters for the Gumbel Distribution", digits = 4) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")


# Take out the optimal parameters
gev_param_df = as.matrix(t(gev_param$mle))
# rownames(gev_param_df) <- c("Optimal Parameters")
# colnames(gev_param_df) <- c("Location", "Scale", "Shape")

# Display a pretty table
# gev_param_df %>% kable(caption = "Optimal Parameters for the Generalized Extreme Value Distribution", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")

# Combine all of the extreme value distributions together
ev_param_df = as.data.frame(rbind(sn_param_df, gum_param_df, gev_param_df))
rownames(ev_param_df) <- c("Skew Normal", "Gumbel", "GEV")
colnames(ev_param_df) <- c("Location", "Scale", "Shape")

# Add a missing value for Gumbel shape
ev_param_df[2,3] <- NA

# Display a pretty table
ev_param_df %>% kable(caption = "Optimal Parameters for the Candidate Distributions", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")


```

To evaluate the model fit more rigorously, I compared the Kullback-Leibler divergence for each theoretical distribution.

```{r}
# First, find the density of the data
m3_den = hist(small_df[,3], breaks = 25, freq = FALSE, plot = FALSE)
# ?hist
# m3_den$density

# Rewrite m2 as the optimal parameters of skew normal
# sn_params = m2$par
# sn_params

# Next compute the probability at each point for the fitted models
## Skew-normal
skn_den = dsnorm(m3_den$mids, mean = m2$par[1], sd = m2$par[2], xi = m2$par[3])
## Gumbel
gumb_den = dgumbel(m3_den$mids, loc = gum_param$mle[1], scale = gum_param$mle[2])
## GEV
gev_den = dgev(m3_den$mids, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])

# Calculate the K-L divergences for each
kl_skn = KLD(m3_den$density, skn_den)
kl_gumb = KLD(m3_den$density, gumb_den)
kl_gev = KLD(m3_den$density, gev_den)

# Combine into a data frame
kl_df = as.data.frame(cbind(kl_skn$mean.sum.KLD, kl_gumb$mean.sum.KLD, kl_gev$mean.sum.KLD))

rownames(kl_df) <- c("Mean Sum K-L Divergence")
colnames(kl_df) <- c("Skew-Normal", "Gumbel", "GEV")

# Display a pretty table
kl_df %>% kable(caption = "Comparing K-L Divergences By Model Fits", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() 
```

```{r}
# Let's see the values of the losses
total_loss_df = as.data.frame(t(colSums(test_loss)))
rownames(total_loss_df) <- c("Losses by Model")
colnames(total_loss_df) <- colnames(v1_2008_alletf[[1]])[-1]

# Subset the data frames
first_4 <- total_loss_df[1,1:4]
last_4 <- total_loss_df[1,5:8]

# Print these out
first_4 %>% kable(caption = "Losses Over the Last 250 Trading Days in 2008 for the Multivariate CAViaR Models", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() 
last_4 %>% kable(caption = "Losses Over the Last 250 Trading Days in 2008 for the Univariate CAViaR Models", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() 
```

Based on the losses for each model during the last 250 trading days in 2008, it looks like the best options are the multivariate CAViaR model without AR terms for the multivariate model class and the symmetric absolute value model for the univariate model class. The full model specifications can be found in the appendix.

Coincidentally, these are among the simplest models available among the models plotted above. A natural criticism of this approach is that the losses are lower for the CAViaR specifications without lagged predictors. This is a fair point, however, the period of 2008 is a period of extreme crisis, and a simpler, ARMA-style model might seem to work better other things equal. Future work can extend this work to other periods. Since these are the best two options during this period of interest, the next step is to find reasonable parametric distributions to model their values.

## Finding Distributions of the Forecasts

To fit the Hidden Markov Model, it is necessary to find distributions that appropriately fit the two models above. While the distribution of predictions from the multivariate model is fairly well-approximated by a normal distribution, the distribution of predictions from the univariate model was highly left-skewed, which makes fitting a distribution difficult.

The first step was to try transformations of the aforementioned predictions from the univariate model, namely $\sqrt{\max(x+1) -x}$, $\log_{10} (\max(x+1) -x)$, and $1/(\max(x+1) -x)$ following the suggestions found here [@Kassambara]. While these did (in some cases) eliminate the left-skew, it often created a more pronounced right-skew! Thus, I attempted to model the data without transformation.

There are many candidate distributions that could be used to model the empirical distribution of values from the univariate CAViaR model, but the three that stood out are:

1. The Skew Normal Distribution (parameterized by location, scale, and shape parameters)
2. The Gumbel Distribution (parameterized by location and scale parameters)
3. The Generalized Extreme Value Distribution (parameterized by location, scale, and shape parameters)



```{r}
# Extract the data
small_df = v1_2008_alletf[[1]][,c(1,2,6)]

# Add 1
# small_df2 = small_df + 1

# These fits don't work very well
# hist(log(small_df2[,2]), breaks = 25)
# hist(small_df2[,3], breaks = 25)
# hist(log(small_df2[,3]), breaks = 25)
# hist((small_df[,3]), breaks = 25)

# small_df2
# ?snormFit

# Let's try another transformation
# t1 = sqrt(max(small_df[,3]+1) - small_df[,3])
# t2 = log10(max(small_df[,3]+1) - small_df[,3])
# t3 = 1/(max(small_df[,3]+1) - small_df[,3])

# Plot the transformations
# hist(t1, breaks = 25)
# hist(t2, breaks = 25)
# hist(t3, breaks = 25)


```


```{r}
# Fit a skew normal
m1 = snormFit(small_df[,2])
m2 = snormFit(small_df[,3])

# m1

# snormFit(small_df[,2]+1)
# snormFit(small_df[,3]+1)
# These may not work very well

# square_val = small_df[,3]^2
# m3 = snormFit(square_val)
# m4 = snormFit(t3)
# m4



# hist((small_df[,3])^2, breaks = 25)
```


```{r}
# Fit a gumbel function
gum_param = gum.fit(small_df[,3], show = FALSE)
# ?gum.fit
# gum_param$mle

# Try fitting a GEV distribution
gev_param = gev.fit(small_df[,3], show = FALSE)
# gev_param$mle

# # Plot histograms
# # Gumbel
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgumbel(xfit, loc = gum_param$mle[1], scale = gum_param$mle[2])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# # GEV
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)

par(mfrow = c(1,2))

# Let's look at the data
h = hist(small_df[,2], breaks = 25, xlab = "Modeled Values", main = "Multivariate CAViaR")
# Overset a density function
xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
yfit = dsnorm(xfit, mean = m1$par[1], sd = m1$par[2], xi = m1$par[3])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,2])
lines(xfit, yfit, col="blue", lwd=2)
# Overset a normal density function
xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
yfit_n = dnorm(xfit, mean = mean(small_df[,2]), sd = sd(small_df[,2]))
yfit_n <- yfit_n*diff(h$mids[1:2])*length(small_df[,2])
lines(xfit, yfit_n, col="red", lwd=2, lty = 2)

# Add a legend
legend("topleft", legend = c("Normal", "Skew Normal"), col = c("blue", "red"), lty = c(1,2), lwd = c(2,2))



# Let's look at the second set of data
h = hist(small_df[,3], breaks = 25, xlab = "Modeled Values", main = "Univariate CAViaR")
# Overset a density function - Skew Normal
xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
yfit = dsnorm(xfit, mean = m2$par[1], sd = m2$par[2], xi = m2$par[3])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
lines(xfit, yfit, col="blue", lwd=2)
# Overset a density function - Gumbel
xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
yfit = dgumbel(xfit, loc = gum_param$mle[1], scale = gum_param$mle[2])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
lines(xfit, yfit, col="red", lwd=2)
# Overset a density function - GEV
xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
lines(xfit, yfit, col="green", lwd=2)

# Add a legend
legend("topleft", legend = c("Skew Normal", "Gumbel", "GEV"), col = c("blue", "red", "green"), lty = c(1,1,1), lwd = c(2,2,2))

# par(mfrow = c(1,3))
# 
# # Let's look at the data
# h = hist(small_df[,2], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
# yfit = dsnorm(xfit, mean = m1$par[1], sd = m1$par[2], xi = m1$par[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,2])
# lines(xfit, yfit, col="blue", lwd=2)
# # Overset a normal density function
# xfit<-seq(min(small_df[,2]),max(small_df[,2]),length=250)
# yfit_n = dnorm(xfit, mean = mean(small_df[,2]), sd = sd(small_df[,2]))
# yfit_n <- yfit_n*diff(h$mids[1:2])*length(small_df[,2])
# lines(xfit, yfit_n, col="red", lwd=2, lty = 2)
# 
# # Let's look at the second set of data
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dsnorm(xfit, mean = m2$par[1], sd = m2$par[2], xi = m2$par[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# # # Let's look at the second set of data
# # h = hist(square_val, breaks = 25)
# # # Overset a density function
# # xfit<-seq(min(square_val),max(square_val),length=250)
# # yfit = dsnorm(xfit, mean = m3$par[1], sd = m3$par[2], xi = m3$par[3])
# # yfit <- yfit*diff(h$mids[1:2])*length(square_val)
# # lines(xfit, yfit, col="blue", lwd=2)
# 
# # Let's look at the second set of data
# h = hist(t3, breaks = 25)
# # Overset a density function
# xfit<-seq(min(t3),max(t3),length=250)
# yfit = dsnorm(xfit, mean = m4$par[1], sd = m4$par[2], xi = m4$par[3])
# yfit <- yfit*diff(h$mids[1:2])*length(t3)
# lines(xfit, yfit, col="blue", lwd=2)

```



```{r}
# # Fit a gumbel function
# gum_param = gum.fit(small_df[,3])
# gum_param$mle
# 
# # Try fitting a GEV distribution
# gev_param = gev.fit(small_df[,3])
# gev_param$mle
# 
# # Plot histograms
# # Gumbel
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgumbel(xfit, loc = gum_param$mle[1], scale = gum_param$mle[2])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# # GEV
# h = hist(small_df[,3], breaks = 25)
# # Overset a density function
# xfit<-seq(min(small_df[,3]),max(small_df[,3]),length=250)
# yfit = dgev(xfit, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])
# yfit <- yfit*diff(h$mids[1:2])*length(small_df[,3])
# lines(xfit, yfit, col="blue", lwd=2)

```

To fit the above histograms,the optimal parameters were fit using maximum likelihood. Note that while there were convergence issues used in fitting the skew-normal distribution, there were not issues with fitting the Gumbel or the GEV distributions.

```{r}
# Below are the optimal parameters for the second model
norm_param_df = as.data.frame(cbind(mean(small_df[,2]),sd(small_df[,2])))
rownames(norm_param_df) <- c("Optimal Parameters")
colnames(norm_param_df) <- c("Mean", "Standard Deviation")

# Display a pretty table
norm_param_df %>% kable(caption = "Optimal Parameters for the Normal Distribution", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "The Mean Was Estimated Using the Sample Mean, SD was Estimated Using Sample Std. Dev.")

# Rewrite m2 as the optimal parameters of skew normal
sn_params = m2$par

# Display the optimal skew-normal parameters
sn_param_df = as.matrix(t(sn_params))
# sn_param_df
# rownames(sn_param_df) <- c("Optimal Parameters")
# colnames(sn_param_df) <- c("Location", "Scale", "Shape")

# Display a pretty table
# sn_param_df %>% kable(caption = "Optimal Parameters for the Skew Normal Distribution", digits = 4) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")

# gum_param$mle

# Display the optimal Gumbel parameters
gum_param_df = as.matrix(t(c(gum_param$mle,0)))
# gum_param_df
# rownames(gum_param_df) <- c("Optimal Parameters")
# colnames(gum_param_df) <- c("Location", "Scale")

# Display a pretty table
# gum_param_df %>% kable(caption = "Optimal Parameters for the Gumbel Distribution", digits = 4) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")


# Take out the optimal parameters
gev_param_df = as.matrix(t(gev_param$mle))
# rownames(gev_param_df) <- c("Optimal Parameters")
# colnames(gev_param_df) <- c("Location", "Scale", "Shape")

# Display a pretty table
# gev_param_df %>% kable(caption = "Optimal Parameters for the Generalized Extreme Value Distribution", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")

# Combine all of the extreme value distributions together
ev_param_df = as.data.frame(rbind(sn_param_df, gum_param_df, gev_param_df))
rownames(ev_param_df) <- c("Skew Normal", "Gumbel", "GEV")
colnames(ev_param_df) <- c("Location", "Scale", "Shape")

# Add a missing value for Gumbel shape
ev_param_df[2,3] <- NA

# Display a pretty table
ev_param_df %>% kable(caption = "Optimal Parameters for the Candidate Distributions", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")


```

To evaluate the model fit more rigorously, I compared the Kullback-Leibler divergence for each theoretical distribution.

```{r}
# First, find the density of the data
m3_den = hist(small_df[,3], breaks = 25, freq = FALSE, plot = FALSE)
# ?hist
# m3_den$density

# Rewrite m2 as the optimal parameters of skew normal
# sn_params = m2$par
# sn_params

# Next compute the probability at each point for the fitted models
## Skew-normal
skn_den = dsnorm(m3_den$mids, mean = m2$par[1], sd = m2$par[2], xi = m2$par[3])
## Gumbel
gumb_den = dgumbel(m3_den$mids, loc = gum_param$mle[1], scale = gum_param$mle[2])
## GEV
gev_den = dgev(m3_den$mids, loc = gev_param$mle[1], scale = gev_param$mle[2], shape = gev_param$mle[3])

# Calculate the K-L divergences for each
kl_skn = KLD(m3_den$density, skn_den)
kl_gumb = KLD(m3_den$density, gumb_den)
kl_gev = KLD(m3_den$density, gev_den)

# Combine into a data frame
kl_df = as.data.frame(cbind(kl_skn$mean.sum.KLD, kl_gumb$mean.sum.KLD, kl_gev$mean.sum.KLD))

rownames(kl_df) <- c("Mean Sum K-L Divergence")
colnames(kl_df) <- c("Skew-Normal", "Gumbel", "GEV")

# Display a pretty table
kl_df %>% kable(caption = "Comparing K-L Divergences By Model Fits", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() 
```

Based on the K-L Divergence, it would seem to make sense to use the generalized extreme value distribution, however, the problem with doing this is the fact that this distribution doesn't have support over the entire real line. Moreover, the Skew-Normal distribution did not work well when fitting the Hidden Markov Model. Therefore, the Gumbel is used in fitting the HMM below.

## HMM Background and Results

The motivating idea behind a Hidden Markov Model is that there are 2 unknown latent states $k$ that generate the data that is seen. (The reference for this information is given here [@Stephens2018]). The algorithm implemented here computes forwards probabilities, $\alpha_{tk} = \mathbb{P}(X_1, ... , X_t; Z_t = k)$. To start, one simply multiplies an equally-weighted prior $\pi_k = 0.5$ by the likelihood of the data given each state, given by $\mathbb{P}(X_1 | Z_1 = k$). 

The likelihood function for the "rational" state (represented by the multivariate CAViaR model) is represented by a normal distribution whereas the likelihood function for the "irrational" state (represented by the univariate CAViaR model) is represented by the Gumbel distribution. Both use the parameters estimated above.

Now, once the $\alpha_1$ value is calculate, $\alpha_2$ is calculated as follows, with a similar process for an arbitrary value $\alpha_t$.

$$
\alpha_2 = (\alpha_1 P)_k \mathbb{P}(X_2 | Z_2 = k)
$$

The $P$ symbol corresponds to a symmetric 2x2 transition matrix where the first row is $(0.9, 0.1)$ and the second row is $(0.1, 0.9)$.


To compute the backwards probabilities, we compute the following $\beta_{tk} = \mathbb{P}(X_{t+1}, ... , X_T; Z_t = k)$, and then the posterior distribution for each state $Z_t$ is given by the following:

$$
\mathbb{P}(Z_t = k| X_1, ..., X_T) = \alpha_{tk} \beta_{tk} / \sum_{k}\alpha_{tk} \beta_{tk}
$$

```{r}
# # Below are the optimal parameters for the second model
# norm_param_df = as.data.frame(cbind(mean(small_df[,2]),sd(small_df[,2])))
# rownames(norm_param_df) <- c("Optimal Parameters")
# colnames(norm_param_df) <- c("Mean", "Standard Deviation")
# 
# # Display a pretty table
# norm_param_df %>% kable(caption = "Optimal Parameters for the Normal Distribution", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "The Mean Was Estimated Using the Sample Mean, SD was Estimated Using Sample Std. Dev.")
# 
# # sn_params
# 
# # Display the optimal skew-normal parameters
# sn_param_df = as.data.frame(t(sn_params))
# rownames(sn_param_df) <- c("Optimal Parameters")
# colnames(sn_param_df) <- c("Location", "Scale", "Shape")
# 
# # Display a pretty table
# sn_param_df %>% kable(caption = "Optimal Parameters for the Skew Normal Distribution", digits = 4) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")
# 
# # gum_param$mle
# 
# # Display the optimal Gumbel parameters
# gum_param_df = as.data.frame(t(gum_param$mle))
# rownames(gum_param_df) <- c("Optimal Parameters")
# colnames(gum_param_df) <- c("Location", "Scale")
# 
# # Display a pretty table
# gum_param_df %>% kable(caption = "Optimal Parameters for the Gumbel Distribution", digits = 4) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")
# 
# 
# # Take out the optimal parameters
# gev_param_df = as.data.frame(t(gev_param$mle))
# rownames(gev_param_df) <- c("Optimal Parameters")
# colnames(gev_param_df) <- c("Location", "Scale", "Shape")
# 
# # Display a pretty table
# gev_param_df %>% kable(caption = "Optimal Parameters for the Generalized Extreme Value Distribution", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "Estimated Using Maximum Likelihood")
```




```{r}
# Clean out old variables
# rm(list=ls())

# This is the true model used in the simulation study. We won't need it.
# set.seed(1)
# T = 200
# K = 2
# sd= 0.4
# P = cbind(c(0.9,0.1),c(0.1,0.9))

# Simulate the latent (Hidden) Markov states
# Also not needed.
# Z = rep(0,T)
# Z[1] = 1
# for(t in 1:(T-1)){
#   Z[t+1] = sample(K, size=1, prob=P[Z[t],])
# }


# Simulate the emitted/observed values
# X= rnorm(T,mean=Z,sd=sd)
# X

# plot(X, main="Realization of HMM; latent states shown in red")
# lines(Z,col=2,lwd=2)

# Instead, let's subset our data frame
# small_test = test_loss[,c(1,5)]
# colSums(small_test)

# Set a value of T and K
T = nrow(small_df)
K = ncol(small_df) - 1

# small_df

# Assign a value of P
P = cbind(c(0.9,0.1),c(0.1,0.9))

# Plot these data
# ?hist
# par(mfrow= c(1,2))
# hist(small_test[,1], breaks = 25)
# hist(small_test[,2], breaks = 25)
# 
# # Transform the dataset
# log_test = log(small_test)
# 
# # Plot the logs?
# par(mfrow= c(1,2))
# h= hist(log_test[,1], breaks = 25)
# # Overset a density function
# xfit<-seq(min(log_test[,1]),max(log_test[,1]),length=40)
# yfit<-dnorm(xfit,mean=mean(log_test[,1]),sd=sd(log_test[,1]))
# yfit <- yfit*diff(h$mids[1:2])*length(log_test[,1])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# # Repeat the same thing
# h= hist(log_test[,2], breaks = 25)
# # Overset a density function
# xfit<-seq(min(log_test[,2]),max(log_test[,2]),length=40)
# yfit<-dnorm(xfit,mean=mean(log_test[,2]),sd=sd(log_test[,2]))
# yfit <- yfit*diff(h$mids[1:2])*length(log_test[,2])
# lines(xfit, yfit, col="blue", lwd=2)
# 
# 
# # Find the means and SDs
# v1 = rbind(mean(log_test[,1]), sd(log_test[,1]))
# 
# # Find the means and SDs
# v2 = rbind(mean(log_test[,2]), sd(log_test[,2]))
# 
# # Store as a data frame
# sum_df = as.data.frame(cbind(v1,v2))
# rownames(sum_df) <- c("Mean", "SD")
# colnames(sum_df) <- c("No AR", "SAV")
# 
# # Format nicely
# sum_df %>% kable(caption = "Summary Statistics for the Log of Losses from Each Model", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling()

# log_test


```




```{r}
# this is the function Pr(X_t | Z_t=k) for our example
# This is the emission probability in the "rational world", modified for whether we miss positive or negative
emit_norm = function(obs,mean, sd){ 
  dnorm(obs, mean = mean, sd = sd)
  # dnorm(x,mean=k,sd=sd)
}

# Here is where we will input our emission probabilities for a GEV
emit_gev = function(obs,loc, scale, shape){ 
  dgev(obs, loc = loc, scale = scale, shape = shape)
}

# The GEV doesn't have support over all of the real line, which is a problem
emit_sn = function(obs,loc, scale, shape){ 
  dsnorm(obs, mean = loc, sd = scale, xi = shape)
}

# Let's try the Gumbel distribution
emit_gumb = function(obs,loc, scale){ 
  dgumbel(obs, loc = loc, scale = scale)
}

# sn_param_df

# Our prior is that both states are equally likely.
prior = c(0.5,0.5) #Assumed prior distribution on Z_1

# The matrix where we store the forwards probabilities
alpha = matrix(nrow = T,ncol=K)


# for(k in 1:K){ 
#   alpha[1,k] = prior[k] * emit(k,X[1])
# }

# head(small_df)
# norm_param_df$`Standard Deviation`
# gev_param_df$Location

# sn_param_df$Location
gum_param_df = ev_param_df[2,1:2]
# gum_param_df$Location

# Initialize alpha[1,]
for(k in 1:K){
  if (k == 1){
    # This is the emission probability assuming normality
    alpha[1,k] = prior[k] * emit_norm(obs = small_df$SPY[1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
  } else if (k == 2){
    # This is the emission probability assuming GEV
    # alpha[1,k] = prior[k] * emit_gev(obs = small_df$SPY[1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
    # This is the emission probability assuming SN
    # alpha[1,k] = prior[k] * emit_sn(obs = small_df$SPY[1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
    # Third time is the charm...assume a Gumbel
    alpha[1,k] = prior[k] * emit_gumb(obs = small_df$SPY[1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
  } else {
    print("k should not be greater than 2")
  }
}

# head(alpha)
# small_df$SPY[1]
# alpha[1,] %*% P


# Forward algorithm
for(t in 1:(T-1)){
  # Find the value of m at each step
  m = alpha[t,] %*% P
  # Loop through to update levels of alpha
  for(k in 1:K){
    # alpha[t+1,k] = m[k]*emit(k,X[t]) what I had
    # alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
    if (k == 1){
      # alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
      alpha[t+1,k] = m[k]*emit_norm(obs = small_df$SPY[t+1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
    } else if (k == 2){
      # Assuming a GEV
      # alpha[t+1,k] = m[k]*emit_gev(obs = small_df$SPY[t+1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
      # Assuming a skew-normal
      # alpha[t+1,k] = m[k]*emit_sn(obs = small_df$SPY[t+1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
      # Assuming a Gumbel
      alpha[t+1,k] = m[k]*emit_gumb(obs = small_df$SPY[t+1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
    } else {
      print("k should not be greater than 2")
    }
    # alpha[t+1,k] = m[k]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k])
  }
}

# emit_gev(obs = small_df$SPY[2], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
# emit_sn(obs = small_df$SPY[2], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
# sn_param_df$Location
# sn_param_df$Scale
# sn_param_df$Shape

# dsnorm(small_df$SPY[2], mean = sn_param_df$Location, sd = sn_param_df$Scale, xi = sn_param_df$Shape)
# hist(rsnorm(10000, mean = sn_param_df$Location, sd = sn_param_df$Scale, xi = sn_param_df$Shape), xlim = c(-2,0))

# Try it with a gumbel
# dgumbel(small_df$SPY[3], loc = gum_param_df$Location, scale = gum_param_df$Scale)

# small_df$SPY[2]

# alpha

```


```{r}
# Initalize a beta matrix
beta = matrix(nrow = T,ncol=K)

# Initialize beta
for(k in 1:K){
  beta[T,k] = 1
}

# Backwards algorithm
for(t in (T-1):1){
  for(k in 1:K){
    # Modify the value of beta
    # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
    # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k]))
    # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
    if (k == 1){
      # Here's what was online
      # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
      # Find the values separately
      v1 = emit_norm(obs = small_df$SPY[t+1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
      v2 = emit_gumb(obs = small_df$SPY[t+1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
      beta[t,k] = sum(beta[t+1,]*P[k,]*c(v1, v2))
    } else if (k == 2){
      # Here's what was online
      # Find the values separately
      v1 = emit_norm(obs = small_df$SPY[t+1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
      v2 = emit_gumb(obs = small_df$SPY[t+1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
      beta[t,k] = sum(beta[t+1,]*P[k,]*c(v1, v2))
    } else {
      print("k should not be greater than 2")
    }
  }
}




```


```{r}
# Lastly, we need to solve for the posterior
ab = alpha*beta
prob = ab/rowSums(ab)
# rowSums(prob)
# prob

plot(prob[,2],type="l",ylim=c(0,1), main="Chance the World is in an Irrational State",lwd=2,ylab="Posterior Probability")
# lines(Z==2,col=2,lwd=2)
```

```{r}

# Bolt on an NA
norm_param_df2 = cbind(norm_param_df, NA)
# Rename rows and columns
rownames(norm_param_df2) <- c("Normal")
colnames(norm_param_df2) <- c("Location", "Scale", "Shape")

# Combine into an optimal parameter df
opt_param_df = rbind(norm_param_df2, ev_param_df)
opt_param_df$Location
opt_param_df

abc = opt_param_df[c("Normal"),]
abc$Location
```

The interpretation of this above graph seems quite clear at first blush - the "hidden" state of the world throughout 2008 is indeed the "irrational" one, marked by the relative success of the univariate CAViaR model. As perhaps with all research, this work generates many more questions than it answers. There are several next steps that are worth exploring.

- Adding more lagged terms into the HMM, or at least tinkering with the transition probability to understand the sensitivity of these parameters to the outcome
- Exploring what happens if the HMM were fit to all 8 candidate models, or using a Neural Network or Random Forest to find the hidden state
- Considering the implicit assumption that these forecasts are based on an asymmetric loss function (see the Appendix), and finding a way to weigh the consideration that an overprediction is a fairly clear indication that the world is not in that state

## Changepoint Detection

The second question is to understand shifts in the economy using a changepoint detection algorithm:

1. Using a set of ETFs, perform Principal Component Analysis at $T$ many points for $M$ many factors - $f_{m,t}$
2. At each time point, add the vectors together to get a resultant: $\sum_{m=1}^M f_{m,t} = r_t$, giving $r_1, r_2, ..., r_T$.
3. Starting with an arbitrary reference point $t_0$ with associated $r_0$ resultant, measure the angle between resultants calculated at different time steps $r_{t}$

$$
\theta_t = \arccos \left(\frac{r_{0} \cdot r_{t}}{||r_{0}||  ||r_{t}||} \right)
$$

The angle $\theta$ could be plotted over time, and changepoints could be detected using Monte Carlo simulation, because PCA transformations are non-linear, so calculating an analytical density from the transformed data is intractable. Moreover, the data fed into the PCA transformation is non-normal, which further supports the notion of using Monte Carlo simulation to establish reasonable estimates of uncertainty for detected changepoints. As with the first line of reasoning, there would certainly be interesting challenges, particularly in creating crisp null and alternative hypotheses. 


## Data Used

The response variable used in this analysis is SPY, which is an exchange-traded fund that aims to track the performance of the S&P 500, which is discussed above. It is broadly used as a bellwether of the U.S. economy, and has the advantage of avoiding survivorship bias - while an individual stock might go bankrupt or merge with another, it is reasonable to assume that these issues do not apply with an ETF. 

Following this logic, there are several classes of response variables used in this analysis. The first group is a set of U.S. sector ETFs obtained from Seeking Alpha [@SeekingAlpha2020]. As with the response variable, these ETFs were publicly traded throughout the Great Recession of 2008.

a. Utilities (XLU)
b. Consumer Staples (XLP)
c. Healthcare (XLV)
d. Technology (XLK)
e. Consumer Discretionary (XLY)
f. Industrial (XLI)
g. Financial Services (XLF)
h. Basic Materials (XLB)
i. Energy (XLE)


The second group for this analysis is bond ETFs. Like the previous two groups, these ETFs potentially contain forward-looking information about the stock market. These ETFs were chosen because they were the first fixed-income ETFs available in the United States, and had enough history for this paper [@NA2017].

a. iShares 1-3 Year Treasury Bond Fund (SHY)
b. iShares 7-10 Year Treasury Bond Fund (IEF)
c. iShares 20+ Year Treasury Bond Fund (TLT)
d. iShares iBoxx $ Investment Grade Corporate Bond ETF (LQD)

## Results



```{r}
# This code below is for use in the CAViaR sections.

rm(list = ls())
```

```{r}
# Here is code that I'll wrap some parts in to avoid superfluous output
quiet <- function(x) { 
  sink(tempfile()) 
  on.exit(sink()) 
  invisible(force(x)) 
} 
```


```{r}
#' This is a function which pulls data for use in the CAViaR model
#'
#' @param symbol - symbol to pull
#' @param compl_case - defaults to true...only includes complete cases in the data
#' @param adj_close - use adjusted closing prices. Default is yes.
#' @param log_return - use log return? Default is yes.
#'
#' @return - a data frame which can be fed into later functions
#' @export
#'
#' @examples - data_pull("SPY")
data_pull = function(symbol, compl_case = 1, adj_close = 1, log_return = 1, start_date = "1900-01-01", end_date = Sys.Date()){
  # Pull in data from quantmod
  response_pull = getSymbols(symbol, auto.assign = FALSE, from = start_date, to = end_date)
  # Get adjusted closing price
  if (adj_close == TRUE){
    df = Ad(response_pull)
  } else {
    df = Cl(response_pull)
  }
  # Return complete cases only 
  if (compl_case == TRUE){
    df = df[complete.cases(df), ]
  } else{
    df = df
  }
  # Calculate log return of data
  if (log_return == TRUE){
    lr = log(df[,1]/shift(df[,1], 1, type = "lag"))
    # Combine data
    df_out = cbind(df, lr)
    # Rename the data 
    colnames(df_out) <- c(sym=symbol, paste0(symbol, "_log_return"))
  } else{
    df_out = df
  }
  # Return data
  return(df_out)
}


```

```{r}
#' Pull the data and run the CAViaR function on it
#'
#' @param input_data - data to use in the function
#' @param range_data - range of the data to use
#'
#' @return - a list of values from the caviar function
#' @export
#'
#' @examples - caviar_pull(spy)
caviar_pull = function(input_data, range_data = (2:dim(input_data)[1])){
  # Run the caviar data
  caviar <- caviarOptim(input_data[range_data,2])
  return(caviar)
}

```


```{r}
#' Function for producing rolling predictions
#' Model 1 = Symmetric Absolute Value, 2 = Asymmetric slope, 3 = Indirect GARCH, 4 = Adaptive
#'
#' @param input_data - input data from the previous function
#' @param range_data - range of the data to consider
#' @param nfcst - number of forecasts to make
#' @param model - model to use (integers 1 through 4). Defaults to 1. 
#' @param level - level of significance to use.
#' @param G - argument for the k parameter in the 4th model (adaptive). Default is 5
#'
#' @return - an xts object which contains rolling CAViaR predictions
#' @export
#'
#' @examples - rolling_predictions(spy, nfcst = 22)
rolling_predictions = function(input_data, range_data = (2:dim(input_data)[1]), nfcst = 250, model =1, level = 0.01, G = 5, col = 2){
  # Run the varpredict function
  varpredict <- rollapplyr(input_data[range_data,col], length(range_data) - nfcst, caviarOptim, model, level, predict = 1, k = G) %>% lag
  # Eliminate NAs
  # pred_no_na = na.omit(varpredict)
  # Return the data
  # return(pred_no_na)
  return(varpredict)
}

```

```{r}
#' Function to Calculate Loss from the above predictions
#'
#' @param symbol - symbol to work with from quantmod. Must be in quotations to work
#' @param start_dt - start date of the data to build the forecast on 
#' @param end_dt - end date of the data to build the forecast on  
#' @param nfcst - number of data points to use in the forecast
#' @param model - model to use. Defaults to 1
#' @param level - level of significance. Defaults to 1%
#' @param G - argument for the k parameter in the 4th model (adaptive). Default is 5
#'
#' @return - loss using absolute value
#' @export - a plot of the data
#'
#' @examples
loss_calc_uv = function(symbol, start_dt, end_dt, nfcst, model = 1, level = 0.01, G = 5){
  # Pull in the data
  raw_data = data_pull(symbol, start_date = start_dt, end_date = end_dt)
  # Forecast based on the data
  fcst = na.omit(rolling_predictions(raw_data, nfcst = nfcst, model = model, level = level, G = G))*(-1)
  # Extract actuals
  act = tail(raw_data, n = nfcst)[,2]
  # Join the two together and rename
  join = merge(fcst,act,all=TRUE)
  colnames(join) <- c("Fcst_VaR", "Act_Return")
  # print(join)
  # Calculate the losses
  loss = abs(sum(ifelse(act > fcst, level, (-1)*(1-level))))
  # Plot the data
  plot = plot.xts(join, col = c("red", "black"), lty = c(2,1), main = "Log Return from the SPY vs. Fcst. VaR",grid.col = NA, legend.loc = "bottomleft")
  return(list(loss, plot, act, fcst))
}

```


```{r, cache = TRUE}
#' This is a function which creates a data frame for the response and explanatory variables that we'll feed into the diffusion index
#'
#' @param symbol_list - a list of symbols recognizable by the getSymbols function
#' @param resp_var - the response variable we'd like to forecast; default is SPY
#' @param compl_case - defaults to true...only includes complete cases in the data
#' @param adj_close - use adjusted closing prices for the explanatory variables? default is 1 for YES
#' @param resp_adj_close - use adjusted closing prices for the explanatory variables? default is 1 for YES
#' @param start_date - starting data to use
#' @param end_date - ending date of the data
#' @param lag_pred - do we lag the predictions? It is STRONGLY recommended that this is 0
#'
#' @return - a data frame which can be fed into the SWfore function
#' @export
#'
#' @examples - diff_index_df(c("XLF", "XLE", "PSCT", "XLV", "VPU", "XLP", "IGF", "XWEB", "PPTY"))
diff_index_df = function(symbol_list, resp_var = "SPY", compl_case = 1, adj_close = 1, resp_adj_close = 1, start_date = "1900-01-01", end_date = Sys.Date(), lag_pred = 1){
  # Pull in response variable
  response_pull = getSymbols(resp_var, auto.assign = FALSE, from = start_date, to = end_date)
  # Get adjusted closing price
  if (resp_adj_close == TRUE){
    diff_df = Ad(response_pull)
  } else {
    diff_df = Cl(response_pull)
  }
  # Loop through the symbols and join in data
  for (i in 1:length(symbol_list)){
    # Pull closing price
    expl_pull = getSymbols(symbol_list[i], auto.assign = FALSE, from = start_date, to = end_date)
    # Extract closing price - 4th element
    if (adj_close == TRUE){
      expl_cl = Ad(expl_pull)
    } else {
      expl_cl = Cl(expl_pull)
    }
    # New code for 4.16.2020 - lag the explanatory variables
    if (lag_pred == TRUE){
      # Lag the explanatory variables by 1
      lag_exp = stats::lag(expl_cl, 1)
      # Append the first lag to the data frame
      diff_df = merge(diff_df, lag_exp, join = "left", fill = NA)
    } else{
      # Return the data frame without lags
      diff_df = merge(diff_df, expl_cl, join = "left", fill = NA)
    }
  }
  if (lag_pred == TRUE){
    # Chop off the first row
    diff_df = diff_df[-1,]
  }
  else {
    print("PLEASE NOTE - the explanatory variables in this DF are NOT lagged. Be careful to avoid look-ahead bias!")
  }
  # Return complete cases only 
  if (compl_case == TRUE){
    diff_df_out = diff_df[complete.cases(diff_df), ]
  } else{
    diff_df_out = diff_df
  }
  
  return(diff_df_out)
}

```

```{r, cache = TRUE}
#' Converts a diff_df into a data frame with approximate percentage changes diff(log(diff_df))
#'
#' @param diff_df - output of the diff_index_df function with complete cases
#'
#' @return - retuns the differenced data
#' @export
#'
#' @examples - pc_diff_index(test_compl) 

pc_diff_index = function(diff_df){
  # Difference the log of the data
  pc_diff_index = diff(log(diff_df))
  # Remove the first row
  pc_diff_index_out = pc_diff_index[-1,]
  return(pc_diff_index_out)
}

```


```{r}
#' Below is the modified diffusion index code.
#'
#' @param y - response variable
#' @param x - predictor variables
#' @param orig - forecast origin
#' @param m - number of diffusion indexes used
#' @param tau - VaR level to use; must be between 0 and 1
#' @param end - specifies an alternate ending value
#' @param print_mdl - print the model summary and the MSE
#'
#' @return - returns a list of variables for use in the diffusion index
#' @export
#'
#' @examples
mod_di = function (y, x, orig, m, tau, end = NULL, print_mdl = 0) 
{
  # Converts the response variables into a matrix
  if (!is.matrix(x)) 
      x = as.matrix(x)
  # nT is number of t time-steps
  nT = dim(x)[1]
  # Add a line to establish the number of data points used in the test.
  if (is.null(end) != TRUE){
    nT = end
  }
  # k is the number of diffusion indices used
  k = dim(x)[2]
  # Sanity checks to ensure that the origin isn't past the number of time points
  if (orig > nT) 
      orig = nT
  # Makes sure that there aren't more predictors than there variables in the dataset
  if (m > k) 
      m = k
  # Makes sure there are at least some variables
  if (m < 1) 
      m = 1
  # Subdivides the dataframe
  x1 = x[1:orig, ]
  # Calculates means of each row
  me = apply(x1, 2, mean)
  # Calculates standard deviations of each column
  se = sqrt(apply(x1, 2, var))
  # Creates a matrix x1, which normalizes all the columns. 
  # This may be an issue since it assumes that the distribution is sufficiently described by the first two moments
  x1 = x
  for (i in 1:k) {
      x1[, i] = (x1[, i] - me[i])/se[i]
  }
  V1 = cov(x1[1:orig, ])
  # Performs an eigen decomposition
  m1 = eigen(V1)
  # Selects eigenvalues
  sdev = m1$values
  # Selects eigenvectors
  M = m1$vectors
  # Makes a smaller matrix
  M1 = M[, 1:m]
  # This is the diffusion index model - [orig x p]*[p x m] = [orig x m]
  Dindex = x1 %*% M1
  # Cut down both the response and predictors to be a reasonable size
  y1 = y[1:orig]
  DF = Dindex[1:orig, ]
  # Apply the linear model - HERE is the key.
  # mm = lm(y1 ~ DF) - old function
  mm = rq(y1 ~ DF, tau = tau)
  # Print the data
  if (print_mdl == 1){
    print(summary(mm))
  }
  # Puts coefficients in a matrix
  coef = matrix(mm$coefficients, (m + 1), 1)
  # Initializes yhat variables and MSE
  yhat = NULL
  MSE = NULL
  if (orig < nT) {
    # Creates a nfcst by (m+1) matrix
    newx = cbind(rep(1, (nT - orig)), Dindex[(orig + 1):nT, 
        ])
    # [nfcstx(m+1)]*[(m+1)x1] = [nfcstx1]
    yhat = newx %*% coef
    # Calculates errors
    err = y[(orig + 1):nT] - yhat
    MSE = mean(err^2)
    if (print_mdl == 1){
      cat("MSE of out-of-sample forecasts: ", MSE, "\n")
    }
  }
  SWfore <- list(coef = coef, yhat = yhat, MSE = MSE, loadings = M1, 
      DFindex = Dindex)
}

```


```{r}
#' Below is the modified diffusion index code to include lagged variables.
#'
#' @param y - response variable
#' @param x - predictor variables
#' @param orig - forecast origin
#' @param m - number of diffusion indexes used
#' @param tau - VaR level to use; must be between 0 and 1
#' @param ar_tf - AR transformation type. (1 - no transformation,
#' 2 - absolute value, 3 - asymmetric slope)
#' @param p - number of AR lags to include. Default is one.
#' @param print_mdl - option to print the model summary to make sure everytning is ok. 0 is default.
#' @param model - model type (1 - SAV, 2 - AS, 3 - GARCH, 4 - ADAPTIVE) 
#'
#' @return - returns a list of variables for use in the diffusion index
#' @export
#'
#' @examples
mod_di_wl = function (y, x, orig, m, tau, ar_tf = 1, p = 1, print_mdl = 0, model = 1, end = NULL) 
{
  # Converts the response variables into a matrix
  if (!is.matrix(x)) 
      x = as.matrix(x)
  # nT is number of t time-steps
  nT = dim(x)[1]
  # Add a line to establish the number of data points used in the test.
  if (is.null(end) != TRUE){
    nT = end
  }
  # k is the number of diffusion indices used
  k = dim(x)[2]
  # Sanity checks to ensure that the origin isn't past the number of time points
  if (orig > nT) 
      orig = nT
  # Makes sure that there aren't more predictors than there variables in the dataset
  if (m > k) 
      m = k
  # Makes sure there are at least some variables
  if (m < 1) 
      m = 1
  # Subdivides the dataframe
  x1 = x[1:orig, ]
  # Calculates means of each row
  me = apply(x1, 2, mean)
  # Calculates standard deviations of each column
  se = sqrt(apply(x1, 2, var))
  # Creates a matrix x1, which normalizes all the columns. 
  # This may be an issue since it assumes that the distribution is sufficiently described by the first two moments
  x1 = x
  for (i in 1:k) {
      x1[, i] = (x1[, i] - me[i])/se[i]
  }
  V1 = cov(x1[1:orig, ])
  # Performs an eigen decomposition
  m1 = eigen(V1)
  # Selects eigenvalues
  sdev = m1$values
  # Selects eigenvectors
  M = m1$vectors
  # Makes a smaller matrix
  M1 = M[, 1:m]
  # This is the diffusion index model - [orig x p]*[p x m] = [orig x m]
  Dindex = x1 %*% M1
  # Cut down both the response and predictors to be a reasonable size
  y1 = y[1:orig]
  DF = Dindex[1:orig, ]
  # Copy the data frame
  DF_wl = Dindex
  # Lag the y-variable
  for (i in 1:p){
    # Create a lagged variable
    lag_var = lag(y, i)
    # Append the first lag to the data frame
    DF_wl = cbind(DF_wl,lag_var)
  }
  # Identify the right columns
  l_ar = ncol(DF_wl)
  f_ar = l_ar - p + 1
  # Keep the last columns kept to the side
  all_lag = DF_wl[,(f_ar:l_ar)]
  # Cut off the first row to avoid NA's
  DF_trim = DF_wl[1:orig,]
  # Rename the columns
  # Here's the new function with an untransformed AR(p) lag
  if (ar_tf == 1){
    # Incorporate everything in to an input data frame
    df_in = cbind(y1[-(1:p)], DF_trim[-(1:p),])
    # Rename the columns
    # Initialize a character vector
    nvec = c(rep(0, 1+m+p))
    # Populate the vector - first value is the response
    nvec[1] <- names(y)
    # Next are the diffusion indices
    for (i in 1:m){
      nvec[i+1] = paste0("Diff_Index_", i)
    }
    # Next are the lagged variables
    for (i in 1:p){
      nvec[i+1+m] = paste0("Lag_", i)
    }
    # Assign the names
    names(df_in) <- nvec
    # Run the model
    mm = rq(df_in[,1] ~ df_in[,-1], tau = tau)
  }
  # Here's the new function with an SAV AR(p) lag
  if (ar_tf == 2){
    # Incorporate everything in to an input data frame
    df_in = cbind(y1[-(1:p)], DF_trim[-(1:p),-(f_ar:l_ar)], abs(DF_trim[-(1:p),(f_ar:l_ar)]))
    # Rename the columns
    # Initialize a character vector
    nvec = c(rep(0, 1+m+p))
    # Populate the vector - first value is the response
    nvec[1] <- names(y)
    # Next are the diffusion indices
    for (i in 1:m){
      nvec[i+1] = paste0("Diff_Index_", i)
    }
    # Next are the lagged variables
    for (i in 1:p){
      nvec[i+1+m] = paste0("Lag_", i)
    }
    # Assign the names. Note that this is a matrix
    names(df_in) <- nvec
    # Run the model
    mm = rq(df_in[,1] ~ df_in[,-1], tau = tau)
  }
  # Here's the new function with an asymmetric slope for the AR(1) lag
  # Indicator; 0 if percent change is negative, 1 if it's positive
  # indi = ifelse(DF_trim[,ar] < 0, 0, 1)
  if (ar_tf == 3){
    # Create a matrix of indicators
    indi_mat = matrix(0, nrow(DF_wl), p)
    # Generalize the above code
    for (i in 1:p){
      # Populate the indicator
      indi_mat[,i] = ifelse(DF_wl[,f_ar + i - 1] < 0, 0, 1)
    }
  }
  # Fitting the regression
  if (ar_tf == 3){
    # Incorporate everything in to an input data frame
    df_in = cbind(y1[-(1:p)], DF_trim[-(1:p),-(f_ar:l_ar)], DF_trim[-(1:p),(f_ar:l_ar)], indi_mat[((p+1):orig),])
    # Rename the columns
    # Initialize a character vector
    nvec = c(rep(0, 1+m+2*p))
    # Populate the vector - first value is the response
    nvec[1] <- names(y)
    # Next are the diffusion indices
    for (i in 1:m){
      nvec[i+1] = paste0("Diff_Index_", i)
    }
    # Next are the lagged variables
    for (i in 1:p){
      nvec[i+1+m] = paste0("Lag_", i)
    }
    # Last are the positive indicator variables
    for (i in 1:p){
      nvec[i+1+m+p] = paste0("Pos_Val_for_Lag_", i)
    }
    # Assign the names. Note that this is a matrix
    names(df_in) <- nvec
    # Run the model
    mm = rq(df_in[,1] ~ df_in[,-1], tau = tau)
    # mm = rq(y1[-(1:p)] ~ DF_trim[-(1:p),-(f_ar:l_ar)] + DF_trim[-(1:p),(f_ar:l_ar)] + indi_mat[((p+1):orig),], tau = tau)
    # Add a different line to account for the indicator variable
    # intercept + m + 2*nlag to account for the number of indicator variables
    coef = matrix(mm$coefficients, (1 + m + 2*p), 1)
  }
  if (print_mdl == 1){
    print(summary(mm))
  }
  # Puts coefficients in a matrix - added the AR terms
  # coef = matrix(mm$coefficients, (m + 1), 1)
  if (ar_tf != 3){
    coef = matrix(mm$coefficients, (1 + m + p), 1)
  }
  # Initializes yhat variables and MSE
  yhat = NULL
  loss = NULL
  if (orig < nT) {
    # Creates a nfcst by (m+2) matrix
    # Add on the lagged variables
    newx = cbind(rep(1, (nT - orig)), Dindex[(orig + 1):nT, ], all_lag[(orig+1):nT,])
    # Incorporate lagged variables
    if (ar_tf == 3){
      newx = cbind(rep(1, (nT - orig)), Dindex[(orig + 1):nT, ], all_lag[(orig+1):nT,], indi_mat[(orig+1):nT,])
    }
    # [nfcstx(m+1)]*[(m+1)x1] = [nfcstx1]
    yhat = newx %*% coef
    # Calculates errors
    loss = abs(sum(ifelse(y[(orig + 1):nT] > yhat, tau, (-1)*(1-tau))))
    # Modifying this part to only print this if specified
    if (print_mdl == 1){
      cat("Losses of out-of-sample forecasts: ", loss, "\n")
    }
  }
  SWfore <- list(coef = coef, yhat = yhat, loss = loss, loadings = M1, 
      DFindex = Dindex, name_vector = nvec)
}

```


```{r}

#' Below is the modified diffusion index code to include lagged variables.
#'
#' @param y - response variable
#' @param x - predictor variables
#' @param orig - forecast origin
#' @param m - number of diffusion indexes used
#' @param tau - VaR level to use; must be between 0 and 1
#' @param ar_tf - AR transformation type. (1 - no transformation,
#' 2 - absolute value, 3 - asymmetric slope)
#' @param p - number of AR lags to include. Default is one.
#' @param print_mdl - option to print the model summary to make sure everytning is ok. 0 is default.
#' @param model - model type (1 - SAV, 2 - AS, 3 - GARCH, 4 - ADAPTIVE) 
#'
#' @return - returns a list of variables for use in the diffusion index
#' @export
#'
#' @examples
pca_at_t = function (y, x, orig, m, tau = 0.01, ar_tf = 1, p = 1, print_mdl = 0, model = 1, end = NULL){
  # Converts the response variables into a matrix
  if (!is.matrix(x)) 
      x = as.matrix(x)
  # nT is number of t time-steps
  nT = dim(x)[1]
  # Add a line to establish the number of data points used in the test.
  if (is.null(end) != TRUE){
    nT = end
  }
  # k is the number of diffusion indices used
  k = dim(x)[2]
  # Sanity checks to ensure that the origin isn't past the number of time points
  if (orig > nT) 
      orig = nT
  # Makes sure that there aren't more predictors than there variables in the dataset
  if (m > k) 
      m = k
  # Makes sure there are at least some variables
  if (m < 1) 
      m = 1
  # Subdivides the dataframe
  x1 = x[1:orig, ]
  # Calculates means of each row
  me = apply(x1, 2, mean)
  # Calculates standard deviations of each column
  se = sqrt(apply(x1, 2, var))
  # Creates a matrix x1, which normalizes all the columns. 
  # This may be an issue since it assumes that the distribution is sufficiently described by the first two moments
  x1 = x
  for (i in 1:k) {
      x1[, i] = (x1[, i] - me[i])/se[i]
  }
  V1 = cov(x1[1:orig, ])
  # Performs an eigen decomposition
  m1 = eigen(V1)
  # Selects eigenvalues
  sdev = m1$values
  # Selects eigenvectors
  M = m1$vectors
  # Makes a smaller matrix
  M1 = M[, 1:m]
  # This is the diffusion index model - [orig x p]*[p x m] = [orig x m]
  Dindex = x1 %*% M1
  # Cut down both the response and predictors to be a reasonable size
  y1 = y[1:orig]
  DF = Dindex[1:orig, ]
  # Copy the data frame
  DF_wl = Dindex
  # Test print
  # print(M)
  DFlist <- list(loadings = M1, DFindex = Dindex)
}

```


```{r}
# # Troubleshooting the data 
df = diff_index_df(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", lag_pred = 1)
pc_df = pc_diff_index(df)
# # pc_df
# 
# # See if we can get all the Global ETFs to work
# df2 = diff_index_df(c("JXI", "KXI", "IXJ", "IXP", "IXN", "RXI", "EXI", "IXG", "MXI", "IXC"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", lag_pred = 1)
# head(df2)
# pc_df2 = pc_diff_index(df2)
# 
# # Test for this function
# vec_1.2 = pca_at_t(y = pc_df2[,1], x = pc_df2[,-1], orig = 757, m = 4)
# 
# # pca_at_t = function (y, x, orig, m, tau, ar_tf = 1, p = 1, print_mdl = 0, model = 1, end = NULL){
# 
# # Run the function
# vec_1 = pca_at_t(y = pc_df[,1], x = pc_df[,-1], orig = 757, m = 4)
# vec_1$loadings
# 
# # Extract the loadings and add
# vec_1_r = rowSums(vec_1$loadings)
# vec_1_r
# 
# # Calculate at a different point
# vec_2 = pca_at_t(y = pc_df[,1], x = pc_df[,-1], orig = 857, m = 4)
# 
# # Extract the loadings and add
# vec_2_r = rowSums(vec_2$loadings)
# vec_2_r
# as.matrix(vec_2_r)
# 
# # Theta calculation
# dotprod = acos( sum(vec_1_r*vec_2_r) / ( sqrt(sum(vec_1_r * vec_1_r)) * sqrt(sum(vec_2_r * vec_2_r)) ) )
# dotprod
```

```{r}

#' A function to compute the angle between two vectors
#'
#' @param x - the first vector 
#' @param y - the second vector 
#'
#' @return - the angle in radians between the vectors
#' @export
#'
#' @examples - angle(vec_1_r, vec_2_r)
angle <- function(x,y){
  # Compute the product of the two vectors
  dot.prod <- x%*%y 
  # Compute the norm
  norm.x <- norm(x,type="2")
  norm.y <- norm(y,type="2")
  # Compute the arccos
  theta <- acos(dot.prod / (norm.x * norm.y))
  # Return theta as a numeric
  theta = as.numeric(theta)
  return(theta)
}

?acos

# abc = angle(c(0.5,2), c(2,0.5))
# cba = angle(c(2,0.5), c(0.5,2))
# abc
# cba
```


```{r}
# pca_at_t = function (y, x, orig, m, tau, ar_tf = 1, p = 1, print_mdl = 0, model = 1, end = NULL){

#' This runs the PCA at time t function
#'
#' @param df - the input data frame
#' @param num_pts - number of points to include in the data frame. A value of 750 means that there are 750 data points to calculate PCA on=
#' @param m - number of indices to use
#' @param ref_pt - which time point do we use as a reference?
#' @param degrees - do we specify the output in degrees or radians?
#'
#' @return - a vector of angles from the reference point 
#' @export
#'
#' @examples - many_pca(df = pc_df, num_pts = 1200, m = 5, ref_pt = 1)
many_pca = function(df, num_pts, m, ref_pt = 1, degrees = TRUE, uni_plot = FALSE){
  # Find the end of the dataframe
  len_df = dim(df)[1]
  # Calculate the number of iterations
  n_iter = len_df - num_pts + 1
  # Initialize a matrix to store the resultants. 
  # Rows are equal to the number of explanatory variables in the data frame
  # Columns are equal to the number of iterations we are going to run
  res_mat = matrix(0, nrow = (ncol(df)-1), ncol = n_iter)
  # Call the function above
  for (i in 1:n_iter){
    # Extract the loading vectors
    load_vecs = pca_at_t(y = df[i:(i+num_pts-1),1], x = df[i:(i+num_pts-1),-1], orig = 999999, m = m)$loadings
    # Calculate the resultants and store
    # print(as.matrix(rowSums(load_vecs)))
    res_mat[,i] = as.matrix(rowSums(load_vecs))
    # Why does this change so much?
  }
  # Initialize a resultant matrix
  # angle_mat = matrix(0, nrow = n_iter, ncol = n_iter)
  angle_vec = rep(0, n_iter)
  # Compute the pairwise comparisons using a double for loop code below.
  # for (i in 1:n_iter){
  # Restrict to one row
  for (j in 1:n_iter){
    # if (i >= j){
      angle_vec[j] = angle(res_mat[,ref_pt], res_mat[,j])
    # } else {
      # res_mat[i,j] <- 
    }
  # }
  # Input an identical zero for the reference point to avoid NaN problems
  angle_vec[ref_pt] = 0
  # Convert to radians
  if (degrees == TRUE){
    angle_vec = angle_vec*(180/pi)
  } 
  # Plot only if there is the option specified
  if (uni_plot == TRUE){
    # Define the start and end date
    start = index(pc_df)[num_pts]
    end = index(pc_df)[len_df]
    plot.ts(angle_vec, type = "l", xlab = paste("Number of Trading Days Since", as.Date(start)), ylab = "Degree Diff. from Reference Resultant", lwd = 1, main = paste("Angle Between Resultants from ", as.Date(start), "to", as.Date(end)), sub = paste("This is run for ", m, " Diffusion Indices"))
  }
  # plot()
  # Establish a maximum and minimum value
  # max_val = max(angle_vec)
  # min_val = min(angle_vec)
  # # Calculate inital and ending time value
  # start = index(df)[ref_pt]
  # end = index(angle_vec)[length(angle_vec)]
  # ind_vals = index(angle_vec) - start
  # Create an initial plot and add lines
  # 4/2/2020 - fixing the index
  # Return the matrix of resultants
  # return(angle_vec)
  many_pca_list <- list(angles = angle_vec, resultants = res_mat)
}
  

```

```{r}
ghi = many_pca(df = pc_df, num_pts = 1250, m = 7, ref_pt = 1)

# nrow(pc_df)
# def = many_pca(df = pc_df, num_pts = 1000, m = 7, ref_pt = 1, uni_plot = TRUE)

# def
# res_1 = def$resultants[,1]
# res_220 = def$resultants[,220]
# 
# ang1 = angle(res_1, res_220)
# ang1*(180/pi)
# pi

# # acos(sum(res_1*res_220)/sqrt((sum(res_1^2))*(sum(res_1^2))))
# # acos(-0.999)*180/pi
# 
# acosdef = many_pca(df = pc_df2, num_pts = 1000, m = 7, ref_pt = 1)
# dim(pc_df2)[1]
# # pc_df2
# # test_ind = index(pc_df)[750:dim(pc_df)[1]]
# 
# 
# plot.ts(def, tyle = "l")

# def
```



```{r}
#' This function runs pca for all m in a range
#'
#' @param symbol_list - stock symbols to use
#' @param num_pts - number of points to include in the data frame. A value of 750 means that there are 750 data points to calculate PCA on
#' @param low_m - low value of m. Defaults to 1
#' @param high_m - high value of m. Defaults to maximum of explanatory vars in dataframe
#' @param ref_pt - which time point do we use as a reference?
#' @param degrees - do we specify the output in degrees or radians?
#' @param resp_var - the response variable we'd like to forecast; default is SPY
#' @param compl_case - defaults to true...only includes complete cases in the data
#' @param adj_close - use adjusted closing prices for the explanatory variables? default is 1 for YES
#' @param resp_adj_close - use adjusted closing prices for the explanatory variables? default is 1 for YES
#' @param start_date - starting data to use
#' @param end_date - ending date of the data
#' @param lag_pred - do we lag the predictions? It is STRONGLY recommended that this is 1
#'
#' @return - ap 
#' @export
#'
#' @examples
many_m_pca = function(symbol_list, num_pts, low_m = NULL, high_m = NULL, ref_pt = 1, degrees = TRUE, resp_var = "SPY", compl_case = 1, adj_close = 1, resp_adj_close = 1, start_date = "1900-01-01", end_date = Sys.Date(), lag_pred = 1, uni_plot = FALSE){
  # Create the data frame
  df = diff_index_df(symbol_list = symbol_list, resp_var = resp_var, start_date = start_date, end_date = end_date, lag_pred = lag_pred, compl_case = compl_case, adj_close = adj_close, resp_adj_close = resp_adj_close)
  # Convert to a percent change
  pc_df = pc_diff_index(df)
  # Find the end of the dataframe
  len_df = dim(pc_df)[1]
  # Calculate the number of iterations
  n_iter = len_df - num_pts + 1
  # Define high and low m if not defined
  if (is.null(low_m) == TRUE){
    low_m = 2
  } 
  if (is.null(high_m) == TRUE){
    high_m = (ncol(pc_df)-1)
  } 
  # Define the number of m's
  no_m = high_m-low_m + 1
  # Initialize a plotting matrix
  plot_matrix = matrix(0, nrow = n_iter, ncol = no_m)
  # print(pc_df)
  # print(many_pca(df = pc_df, num_pts = num_pts, m = 1, ref_pt = ref_pt))
  # Initalize a count dummy
  count = 1
  # Loop through and calculate the angles
  for (j in low_m:high_m){
    # Run the code
    plot_matrix[,count] = many_pca(df = pc_df, num_pts = num_pts, m = j, ref_pt = ref_pt, uni_plot = uni_plot)$angles
    # Add one to the counter
    count = count + 1
  }
  # Plot the data
  # Define the start and end date
  start = index(pc_df)[num_pts]
  end = index(pc_df)[len_df]
  # ind_vals = index(pc_df) - start
  # print(c(start, end, ind_vals))
  # Find min and maxum values
  max_val = max(plot_matrix[,1:ncol(plot_matrix)])
  min_val = min(plot_matrix[,1:ncol(plot_matrix)])
  # # Create an initial plot and add lines
 for (i in 1:ncol(plot_matrix)){
    if (i == 1){
      # Initialize the plot
      plot.ts(plot_matrix[,i], type = "l", xlab = paste("Number of Trading Days Since", as.Date(start)), ylab = "Degree Diff. from Ref. Resultant for Several M", ylim = c(min_val,max_val), lwd = 1, lty = 2, main = paste("Angle Between Resultants from", as.Date(start), "to", as.Date(end)))
    } else if(i %in% seq(2,8,1)) {
        lines(plot_matrix[,i], col = i, lty = 2)
    } else {
        lines(plot_matrix[,i], col = i, lty = 2)
    }
  }
  # # Define a sequence for plotting
  plot_seq = seq(1, ncol(plot_matrix))
  # # Modifying the code below to fix the legend on 7.31.2020
  legend("topleft", legend = c(seq(low_m,high_m,1)), col = plot_seq, lty = 2, lwd = 2)
  # plot.ts(angle_vec, type = "l", xlab = paste("Number of Trading Days Since", as.Date(start)), ylab = "Degree Diff. from Reference Resultant", lwd = 1, main = paste("Angle Between Resultants from ", as.Date(start), "to", as.Date(end)))
  # Return the vector of angles
  return(list(plot_matrix = plot_matrix, pc_df= pc_df))
}


# diff_index_df = function(symbol_list, resp_var = "SPY", compl_case = 1, adj_close = 1, resp_adj_close = 1, start_date = "1900-01-01", end_date = Sys.Date(), lag_pred = 1){
```

```{r}
# # Plotting the data
# us_etfs = many_m_pca(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", lag_pred = 1, num_pts = 750, ref_pt = 1)
# # us_etfs = many_m_pca(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", lag_pred = 1, num_pts = 750, ref_pt = 1)
# plot.ts(us_etfs[,1], type = "l")
# lines(us_etfs[,2], type = "l")
# 
# for (i in 1:ncol(us_etfs)){
#   if (i == 1){
#     # Initialize the plot
#     plot.ts(us_etfs[,i], type = "l", xlab = paste("Number of Trading Days Since", as.Date(start)), ylab = "Degree Diff. from Ref. Dt. for Different M", lwd = 1, main = paste("Angle Between Resultants from", as.Date(start), "to", as.Date(end)))
#   } else if(i %in% seq(2,8,1)) {
#       lines(us_etfs[,i], col = i, lty = 2)
#   } else {
#       lines(us_etfs[,i], col = i, lty = 2)
#   }
# }
# # colnames(us_etfs)
# plot_seq = seq(1, ncol(us_etfs))
# # # Modifying the code below to fix the legend on 7.31.2020
# legend("topleft", legend = c(seq(2,6,1)), col = plot_seq, lty = 2, lwd = 2)
# 
# start = index(pc_df)[1000]
# end = index(pc_df)[1250]
# ind_vals = index(pc_df) - start
# ind_vals
```

```{r}
# all_etfs_08
```


# U.S. ETFs Results for 2007 - 2008

```{r}
# Call the above function
us_etfs_08 = many_m_pca(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", lag_pred = 1, num_pts = 750, ref_pt = 1, low_m = 4)

# is.matrix(us_etfs_08)

```

A natural criticism of the above fits are that the data is noisy. A complication of this analysis is the fact that there isn't necessarily anything to "pin" the data to, because the problem is unsupervised. As such, I think a decent way to picking the wheat from the chaff (or the signal from the noise) is to apply some smoothing filters to the above data. Two options to do so are the moving average smoother and the Lowess smoothers. 

```{r}
#' A function to smooth the data
#'
#' @param plot_mat_in - input plot data
#' @param col_num - col name to reference
#' @param pc_df_in - the input percentage change dataset
#' @param num_pts - number of points used to calculate PCA
#'
#' @return 
#' @export
#'
#' @examples - plot_smooth(plot_mat_in = us_etfs_08$plot_matrix, col_num = 4, pc_df_in = us_etfs_08$pc_df, num_pts = 750)
plot_smooth = function(plot_mat_in, col_num, pc_df_in, num_pts, low_m){
  # Extract the relevant column
  plot_vec = plot_mat_in[,col_num]
  # Find the end of the dataframe
  len_df = dim(pc_df)[1]
  # Run an MA smoother on the data - Plus or minus 5
  wgts_pm5 = c(rep(1,11))/11
  ma_vec_pm5 = stats::filter(plot_vec, sides=2, filter=wgts_pm5)
  # Plus or minus 10
  wgts_pm10 = c(rep(1,21))/21
  ma_vec_pm10 = stats::filter(plot_vec, sides=2, filter=wgts_pm10)
  # Run a Lowess smoother
  lowess_vec = lowess(plot_vec, f = 1/6)
  # Define the start and end date
  start = index(pc_df)[num_pts]
  end = index(pc_df)[len_df]
  # Find the value of m
  true_m = low_m + (col_num - 1)
  # Plot the data
  plot.ts(plot_vec, type = "l", xlab = paste("Number of Trading Days Since", as.Date(start)), ylab = paste0("Degree Diff. from Ref. Resultant for M = ", 7), ylim = c(0,180), lwd = 1, lty = 2, main = paste("Angle Between Resultants from", as.Date(start), "to", as.Date(end)))
  # Add lines to the plot
  lines(ma_vec_pm5, col = 3, lwd = 2, lty = 2)
  lines(ma_vec_pm10, col = 4, lwd = 2, lty = 2)
  lines(lowess_vec, col = 5, lwd = 2, lty = 2)
  # Add a line at 90
  abline(h = 90, col = 2, lwd = 1, lty = 2)
  # Add a legend
  legend("topleft", legend = c("Raw Series", "MA +/- 5 Days", "MA +/- 10 Days", "Lowess with f = 1/6", "90 Degrees"), col = c(1,3,4,5,2), lty = c(2,2,2,2,2), lwd = c(1,2,2,2,1))
}
```

```{r}
plot_smooth(plot_mat_in = us_etfs_08$plot_matrix, col_num = 4, pc_df_in = us_etfs_08$pc_df, num_pts = 750, low_m = 4)


```

In the above plot, I applied a few filters, namely the moving average filter with equal weights for plus/minus 5 days and 10 days as well as the Lowess with a weight of $f = 1/6$ for higher levels of precision. Below are the specifications for moving average with 5 and 10 days, where $x_t$ is the angle between the resultants.

$$
m_{t,5} = \sum_{j =-5}^5 \frac{1}{11} x_{t-j}, m_{t,10} = \sum_{j =-10}^{10} \frac{1}{21} x_{t-j}
$$


The Lowess smoother is a smoother that per Shumway and Stoffer [@Shumway2016], is a technique "based on k-nearest neighbors regression, wherein one uses only the data $[x_{t-k/2}, ... , x_t, ..., x_{t+k/2}]$ to predict the true value of $x_t$. Based on a visual inspection of the data, it appears that a more precise estimator was in order, so the Lowess function only uses 1/6th of the data.

There are some interesting trends that bear discussion. While some of macroeconomics may appear to be little more than a crystal ball, or a science based upon strong priors, I don't believe that to be the case here. In the above graph, the angle between resultant vectors calculated from baskets of equity exchange-traded funds (ETFs) for different sectors of the U.S. economy point, almost as a leading indicator, towards the once-in-a-lifetime tumult that was about to grip the U.S. and global financial markets. Indeed, in the graph above, the angle of the resultant seems to stabilize for the month of August 2008, which corresponds roughly to the 400 – 425 trading days since the reference point at the end of 2006. This is before Lehman Brothers declared bankruptcy, before AIG was bailed out, and before Congress passed TARP. It remains to be seen whether this is coincidental or whether there really is a leading indicator here, but I believe the question is worth asking and worth exploring further.

From a statistical perspective, the most important fact about these trends is that the algorithm that produces them, namely PCA, is *an unsupervised learning algorithm*. At no point in this work is there a model of any kind trying to predict gyrations in the S&P500 or the Dow Jones or the real economy. Indeed, remember that these principal components are based upon sector ETFs, things like Energy, Consumer Staples, and Utilities, which in turn are based upon individual stocks - companies like Exxon, Capital One, General Electric, and Amazon. These stock prices may seemingly be enigmatic and noisy, but ironically they look this way because according to economic theory, they are likely to contain all the relevant information about their company as determined by market participants [@Malkiel2003]. What if, in all of their foresight, they saw the most cataclysmic economic event of our time before it happened, without even aiming to do so?

While future work in this area would benefit greatly from a variational autoencoder to be able to figure out what is exactly a meaningful change, it may also benefit from having additional data fed into it.


```{r}
# plot
# # us_etfs_08[,4]
# lowess(us_etfs_08[,4])
# 
# for (i in 1:ncol(plot_matrix)){
#     if (i == 1){
#       # Initialize the plot
#       plot.ts(plot_matrix[,i], type = "l", xlab = paste("Number of Trading Days Since", as.Date(start)), ylab = "Degree Diff. from Ref. Resultant for Several M", ylim = c(min_val,max_val), lwd = 1, lty = 2, main = paste("Angle Between Resultants from", as.Date(start), "to", as.Date(end)))
#     } else if(i %in% seq(2,8,1)) {
#         lines(plot_matrix[,i], col = i, lty = 2)
#     } else {
#         lines(plot_matrix[,i], col = i, lty = 2)
#     }
#   }

```





# Appendix


## U.S. ETFs Results for the 2011 - 2012

```{r}
# Call the above function
us_etfs_12 = many_m_pca(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2008-01-01", end_date = "2012-12-31", lag_pred = 1, num_pts = 750, ref_pt = 1, low_m = 4)
```

## U.S. ETFs Results for the 2013 - 2014


```{r}
# Call the above function
us_etfs_14 = many_m_pca(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2010-01-01", end_date = "2014-12-31", lag_pred = 1, num_pts = 750, ref_pt = 1, low_m = 4)
```

## U.S. ETFs Results for the 2015 - 2016


```{r}
# Call the above function
us_etfs_16 = many_m_pca(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2012-01-01", end_date = "2016-12-31", lag_pred = 1, num_pts = 750, ref_pt = 1, low_m = 4)

```





## U.S. + Bond ETF Results for 2007 - 2008





```{r}
# Call the above function
all_etfs_08 = many_m_pca(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE", "SHY", "IEF", "TLT", "LQD"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", lag_pred = 1, num_pts = 750, ref_pt = 1, low_m = 8)

# us_etfs_08
```

## U.S. + Bond ETF Results for the 2011 - 2012

```{r}
# Call the above function
all_etfs_12 = many_m_pca(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE", "SHY", "IEF", "TLT", "LQD"), resp_var = "SPY", start_date = "2008-01-01", end_date = "2012-12-31", lag_pred = 1, num_pts = 750, ref_pt = 1, low_m = 8)
```

## U.S. + Bond ETF Results for the 2013 - 2014


```{r}
# Call the above function
all_etfs_14 = many_m_pca(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE", "SHY", "IEF", "TLT", "LQD"), resp_var = "SPY", start_date = "2010-01-01", end_date = "2014-12-31", lag_pred = 1, num_pts = 750, ref_pt = 1, low_m = 8)
```

## U.S. + Bond ETF Results for the 2015 - 2016


```{r}
# Call the above function
all_etfs_16 = many_m_pca(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE", "SHY", "IEF", "TLT", "LQD"), resp_var = "SPY", start_date = "2012-01-01", end_date = "2016-12-31", lag_pred = 1, num_pts = 750, ref_pt = 1, low_m = 8)
```


## Univariate CAViaR Model Specifications

However, work needed to be done to align the diffusion index model with the CAViaR model, which is defined below. The following variables are required for use in the CAViaR model. For ease of notation, these are sourced directly from the Engle and Manganelli 2004 CAViaR paper [@Engle2004], with some added description:

- $(y_t)_{t=1}^T$ is a "vector of portfolio returns"
- $\theta$ is the "probability associated with VaR" (a 5% VaR would mean $\theta = 0.05$)
- $\boldsymbol{x_t}$ is a "vector of time $t$ observable variables"
- $f_t(\boldsymbol{\beta}) \equiv f_t(\boldsymbol{x_{t-1}, \boldsymbol{\beta_\theta}})$ is the "time $t$", "$\theta$ quantile of the distribution of portfolio returns formed at time $t-1$"

The authors then describe a "generic CAViaR specification" as follows:

$$
f_t(\boldsymbol{\beta}) = \beta_0 + \sum_{i=1}^q \beta_i f_{t-1}(\boldsymbol{\beta}) + \sum_{j=1}^r \beta_{q+j} l(\boldsymbol{x_{t-j}})
$$

What is interesting about the general setup is that there are two main components to the model - lagged observed variables (represented by $l$) and lagged values of unknown parameters, which in the specification below is used as moving average terms. As such, it is reasonable to generalize the specifications below as nonlinear ARMA models where $y_{t-1}$ terms refer to previous returns, whereas $f_{t-1}(\beta_1)$ terms refer to previous predictions.


### Adaptive CAViaR Model

Consider the following model:

$$
f_t(\beta_1) = f_{t-1}(\beta_1) + \beta_1\left[\left(1+ \exp(G[y_{t-1} - f_{t-1}(\beta_1)])  \right)^{-1} - \theta \right] 
$$

Following Engle and Manganelli's 2004 paper, we choose $G = 10$, so that is what is used in the results section of this paper. The authors state the reason for the seemingly arbitrary choice is that while "the parameter G itself could be estimated; however, this would go against the spirit of this model, which is simplicity". Previous sensitivity analysis showed that running the adaptive model with $G = 5$ did not materially affect the VaR predictions - the accuracy was not changed. While this model is nonlinear in G and total scale invariance in $G$ would be surprising given the nonlinear relationship, the fact that the other fitted parameters likely adjusted is not surprising.

### Symmetric Absolute Value CAViaR Model

Below is the symmetric absolute value CAViaR model:

$$
f_t(\boldsymbol{\beta}) = \beta_1 + \beta_2f_{t-1}(\boldsymbol{\beta}) + \beta_3|y_{t-1}|.
$$

### Asymmetric Slope CAViaR Model

Below is the asymmetric slope CAViaR model:

$$
f_t(\boldsymbol{\beta}) = \beta_1 + \beta_2f_{t-1}(\boldsymbol{\beta}) + \beta_3(y_{t-1})^+ + \beta_4(y_{t-1})^-.
$$

### Indirect GARCH (1,1) CAViaR Model

Below is the Indirect GARCH (1,1) model:

$$
f_t(\boldsymbol{\beta}) = (\beta_1 + \beta_2f_{t-1}^2(\boldsymbol{\beta}) + \beta_3y_{t-1}^2)^{1/2}.
$$

## Multivariate CAViaR Model Specifications

The multivariate CAViaR model takes inspiration from the models described above in several specifications, as mentioned in the original specifications. The general model form looks like the specification below:

$$
f_t(\boldsymbol{\beta}) = \beta_0 +\sum_{i=1}^p \beta_i y_{t-i} + \sum_{j=1}^m \beta_{j+p} f_{j,t-1} + e_t.
$$

As with the univariate CAViaR model, the object of interest is a $\theta$ percentile return and the model is fit iteratively to minimize the loss function on the training data. However, there are some notable differences between the univariate model and the multivariate model. First, there are no moving average terms (lagged error terms) - the reasoning for this is because this model aims for a clear economic interpretation, and crisp interpretations of MA models are harder to create. Also, moving average models require recursive estimation since error terms are not observed, and so developing a method to work with these errors in a robust regression framework is challenging.

Second, in some of the specifications below, there are lagged return variables. This is similar to the univariate CAViaR specification, though there is often more than 1 lag as in the univariate model - there are $p$ lags in the dataset. Third, in all of the specifications below, there are $m$ diffusion indices used in each model lagged by one time step to avoid look-ahead bias.


### Multivariate CAViaR: No Lags Model

$$
f_t(\boldsymbol{\beta}) = \beta_0 +\sum_{j=1}^m \beta_{j} f_{j,t-1} + e_t
$$

### Multivariate CAViaR with Autoregressive Terms Added

$$
f_t(\boldsymbol{\beta}) = \beta_0 +\sum_{i=1}^p \beta_i y_{t-i} + \sum_{j=1}^m \beta_{j+p} f_{j,t-1} + e_t
$$

### Multivariate CAViaR with Symmetric Absolute Value Autoregressive Terms Added

$$
f_t(\boldsymbol{\beta}) = \beta_0 +\sum_{i=1}^p \beta_i |y_{t-i}| + \sum_{j=1}^m \beta_{j+p} f_{j,t-1} + e_t
$$

### Multivariate CAViaR with Asymmetric Slope Autoregressive Terms Added

$$
f_t(\boldsymbol{\beta}) = \beta_0 +\sum_{i=1}^p \beta_i (y_{t-i})_+ + \sum_{j=p+1}^{2p} \beta_i (y_{t-i})_- + \sum_{k=1}^m \beta_{k+2p} f_{k,t-1} + e_t
$$


## Fitting the Models

To fit the models, an optimal value of $m$ diffusion indices and $p$ autoregressive terms are added (or $2p$ in the case of the asymmetric slope model). The optimal values of these parameters are determined using a validation dataset. In all of the runs below, there are a total of 5 years of trading days, or about 1,260 days assuming 252 trading days a year. The adjusted closing prices are logged and differenced, shortening the dataset by one. After doing this, the last 250 data points are reserved as test data, and the 250 data points before that are used as a validation set. Measured by the loss function written out below, the values of $p$ and $m$ that minimize losses are chosen and the optimal model is refit over both the training and the validation data combined and then evaluated on the test data. Note that there is an optimal model which is chosen for each of the four multivariate CAViaR specifications described above, so there are 4 optimal sets of $p$ and $m$ chosen for each set of models. Thus, there are 8 models compared on the test data - 4 univariate CAViaR models and 4 multivariate CAViaR models.

From the CAViaR paper, the $\theta$th regression quantile is defined as any $\boldsymbol{\hat{\beta}}$ that solves the following loss function:

$$
\overset{argmin}\beta \frac{1}{T}\sum_{t=1}^T [\theta - I(y_t < f_t(\boldsymbol{\beta}))][y_t - f_t(\boldsymbol{\beta})]
$$

# Code

The code can be found at the location listed below in the "STAT_771_Class_Project.Rmd" file.

https://github.com/stevenmoen/stat_771_final_project

# Big HMM Function

```{r, eval = FALSE}
# Now, we need to write everything as a big function

hmm_big = function(df, hist_plots = TRUE, P_alt = NULL){
  # Subset our data frame using which columns
  # small_test = test_loss[,which_cols]
  # Set a value of T and K
  T = nrow(df)
  K = ncol(v)
  # Assign a value of P
  if (is.null(P_alt)== TRUE){
    # Assign a 90% chance of staying put
    P = cbind(c(0.9,0.1),c(0.1,0.9))
  } else{
    # Stay where you are
    P = P_alt
  }
  # Transform the dataset
  # log_test = log(small_test)
  # Plot the data
  # if (hist_plots == TRUE){
  #   # Plot these data
  #   par(mfrow= c(2,2))
  #   # Transform the dataset
  #   log_test = log(small_test)
  #   # First, the un-transformed histograms
  #   hist(small_test[,1], breaks = 25)
  #   hist(small_test[,2], breaks = 25)
  #   # Plot the data with an overset histogram
  #   h= hist(log_test[,1], breaks = 25)
  #   # Overset a density function
  #   xfit<-seq(min(log_test[,1]),max(log_test[,1]),length=40)
  #   yfit<-dnorm(xfit,mean=mean(log_test[,1]),sd=sd(log_test[,1]))
  #   yfit <- yfit*diff(h$mids[1:2])*length(log_test[,1])
  #   lines(xfit, yfit, col="blue", lwd=2)
  #   # Repeat the same thing
  #   h= hist(log_test[,2], breaks = 25)
  #   # Overset a density function
  #   xfit<-seq(min(log_test[,2]),max(log_test[,2]),length=40)
  #   yfit<-dnorm(xfit,mean=mean(log_test[,2]),sd=sd(log_test[,2]))
  #   yfit <- yfit*diff(h$mids[1:2])*length(log_test[,2])
  #   lines(xfit, yfit2, col="blue", lwd=2)
  # }
  # Find the means and SDs
  # v1 = rbind(mean(log_test[,1]), sd(log_test[,1]))
  # Find the means and SDs
  # v2 = rbind(mean(log_test[,2]), sd(log_test[,2]))
  # Store as a data frame
  # sum_df = as.data.frame(cbind(v1,v2))
  # rownames(sum_df) <- c("Mean", "SD")
  # colnames(sum_df) <- c("No AR", "SAV")
  # Format nicely
  # print(sum_df %>% kable(caption = "Summary Statistics for the Log of Losses from Each Model", digits = 2) %>% kable_styling("striped", full_width = F) %>% kable_styling())
  # Now run the HMM
  # this is the function Pr(X_t | Z_t=k) for our example
  # We need to modify this
    # This is the emission probability in the "rational world", modified for whether we miss positive or negative
  emit_norm = function(obs,mean, sd){ 
    dnorm(obs, mean = mean, sd = sd)
  }
  # # Here is where we will input our emission probabilities for a GEV
  # emit_gev = function(obs,loc, scale, shape){
  #   dgev(obs, loc = loc, scale = scale, shape = shape)
  # }
  # 
  # # The GEV doesn't have support over all of the real line, which is a problem
  # emit_sn = function(obs,loc, scale, shape){ 
  #   dsnorm(obs, mean = loc, sd = scale, xi = shape)
  # }
  # Let's try the Gumbel distribution
  emit_gumb = function(obs,loc, scale){ 
    dgumbel(obs, loc = loc, scale = scale)
  }
  # Our prior is that both states are equally likely.
  prior = c(0.5,0.5) #Assumed prior distribution on Z_1
  # The matrix where we store the forwards probabilities
  alpha = matrix(nrow = T,ncol=K)
  # Initialize alpha[1,]
  for(k in 1:K){
    if (k == 1){
      # This is the emission probability assuming normality
      alpha[1,k] = prior[k] * emit_norm(obs = df$SPY[1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
    } else if (k == 2){
      # This is the emission probability assuming GEV
      # alpha[1,k] = prior[k] * emit_gev(obs = small_df$SPY[1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
      # This is the emission probability assuming SN
      # alpha[1,k] = prior[k] * emit_sn(obs = small_df$SPY[1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
      # Third time is the charm...assume a Gumbel
      alpha[1,k] = prior[k] * emit_gumb(obs = df$SPY[1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
    } else {
      print("k should not be greater than 2")
    }
  }
  # Forward algorithm
  for(t in 1:(T-1)){
    # Find the value of m at each step
    m = alpha[t,] %*% P
    # Loop through to update levels of alpha
    for(k in 1:K){
      # alpha[t+1,k] = m[k]*emit(k,X[t]) what I had
      # alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
      if (k == 1){
        # alpha[t+1,k] = m[k]*emit(k,X[t+1]) online
        alpha[t+1,k] = m[k]*emit_norm(obs = df$SPY[t+1], mean = norm_param_df$Mean, sd = norm_param_df$`Standard Deviation`)
      } else if (k == 2){
        # Assuming a GEV
        # alpha[t+1,k] = m[k]*emit_gev(obs = small_df$SPY[t+1], loc = gev_param_df$Location, scale = gev_param_df$Scale, shape = gev_param_df$Shape)
        # Assuming a skew-normal
        # alpha[t+1,k] = m[k]*emit_sn(obs = small_df$SPY[t+1], loc = sn_param_df$Location, scale = sn_param_df$Scale, shape = sn_param_df$Shape)
        # Assuming a Gumbel
        alpha[t+1,k] = m[k]*emit_gumb(obs = df$SPY[t+1], loc = gum_param_df$Location, scale = gum_param_df$Scale)
      } else {
        print("k should not be greater than 2")
      }
      # alpha[t+1,k] = m[k]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k])
    }
  }
  # Initalize a beta matrix
  beta = matrix(nrow = T,ncol=K)
  # Initialize beta
  for(k in 1:K){
    beta[T,k] = 1
  }
  # Backwards algorithm
  for(t in (T-1):1){
    for(k in 1:K){
      # Modify the value of beta
      # beta[t,k] = sum(beta[t+1,]*P[k,]*emit(1:K,X[t+1]))
      beta[t,k] = sum(beta[t+1,]*P[k,]*emit(obs = log_test[t+1,k], mean = sum_df[1,k], sd = sum_df[2,k]))
    }
  }

  
}

```

# Literature Cited